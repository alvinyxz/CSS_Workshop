frequency),
y = frequency)) +
# reorder the words by frequency,
# so that the most frequent words appear at the top
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 50 Words in Congress Posts",
x = "Words", y = "Frequency") +
theme_minimal()
ggsave("Congress_Top50.pdf",
plot = last_plot(),
width = 10,
height = 8)
### PROCESS AGENCY DATASET ###
# Load Agency data
Agency <- read.csv("Agency.csv")
# Explore the dataset
head(Agency)
names(Agency)
# Create corpus and incorporate some document variables
Corpus_Agency <- corpus(Agency$cleanedtext,
docvars = Agency[, c("Page.Name",
"User.Name",
"Facebook.Id",
"Post.Created",
"Comments",
"Shares")])
# Tokenize and clean
Corpus_Agency_tokens <- tokens(Corpus_Agency,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
split_hyphens = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("en")) %>%
tokens_replace(pattern = lexicon::hash_lemmas$token,
replacement = lexicon::hash_lemmas$lemma) %>%
tokens_remove(c("de", "also", "may", "include", "can",
"p.m", "a.m", "va", "st", "pm", "datum",
"covid", "vaccination", "vaccinate", "vaccine", "coronavirus",
"2")) %>%
tokens_replace(pattern = "numb", replacement = "number")
# Build document-feature-matrix and compute top features
Corpus_Agency_dfm <- Corpus_Agency_tokens %>%
dfm()
topfeatures_Agency <- textstat_frequency(Corpus_Agency_dfm, n = 75)
topfeatures_Agency
topfeatures_Agency_features <- topfeatures_Agency$feature
# Build co-occurrence matrix (FCM)
Corpus_Agency_tokens_fcm <- fcm(Corpus_Agency_tokens,
context = "document",
tri = FALSE)
Corpus_Agency_tokens_fcm_trimmed <- fcm_select(Corpus_Agency_tokens_fcm,
pattern = topfeatures_Agency_features)
# Plot and save network
set.seed(123) # for reproducibility
pdf("Agency_Network.pdf", width = 10, height = 10)
textplot_network(Corpus_Agency_tokens_fcm_trimmed,
min_freq = 0.5,
edge_alpha = 0.25,
edge_size = 2)
dev.off()
# Plot top 50 words as a bar chart
top50_Agency <- textstat_frequency(Corpus_Agency_dfm, n = 50)
ggplot(top50_Agency,
aes(x = reorder(feature,
frequency),
y = frequency)) +
# reorder the words by frequency,
# so that the most frequent words appear at the top
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 50 Words in Agency Posts",
x = "Words", y = "Frequency") +
theme_minimal()
ggsave("Agency_Top50.pdf",
plot = last_plot(),
width = 10,
height = 8)
### PROCESS HOSPITAL DATASET ###
# Load Hospital data
Hospital <- read.csv("Hospital.csv")
# Explore the dataset
head(Hospital)
names(Hospital)
# Create corpus and incorporate some document variables
Corpus_Hospital <- corpus(Hospital$cleanedtext,
docvars = Hospital[, c("Page.Name",
"User.Name",
"Facebook.Id",
"Post.Created",
"Comments",
"Shares")])
# Tokenize and clean
Corpus_Hospital_tokens <- tokens(Corpus_Hospital,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
split_hyphens = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("en")) %>%
tokens_replace(pattern = lexicon::hash_lemmas$token,
replacement = lexicon::hash_lemmas$lemma) %>%
tokens_remove(c("de", "also", "may", "include", "can",
"p.m", "a.m", "va", "st", "pm", "datum",
"covid", "vaccination", "vaccinate", "vaccine", "coronavirus",
"2")) %>%
tokens_replace(pattern = "numb", replacement = "number")
# Build document-feature-matrix and compute top features
Corpus_Hospital_dfm <- Corpus_Hospital_tokens %>%
dfm()
topfeatures_Hospital <- textstat_frequency(Corpus_Hospital_dfm, n = 75)
topfeatures_Hospital
topfeatures_Hospital_features <- topfeatures_Hospital$feature
# Build co-occurrence matrix (FCM)
Corpus_Hospital_tokens_fcm <- fcm(Corpus_Hospital_tokens,
context = "document",
tri = FALSE)
Corpus_Hospital_tokens_fcm_trimmed <- fcm_select(Corpus_Hospital_tokens_fcm,
pattern = topfeatures_Hospital_features)
# Plot and save network
set.seed(123) # for reproducibility
pdf("Hospital_Network.pdf", width = 10, height = 10)
textplot_network(Corpus_Hospital_tokens_fcm_trimmed,
min_freq = 0.5,
edge_alpha = 0.25,
edge_size = 2)
dev.off()
# Plot top 50 words as a bar chart
top50_Hospital <- textstat_frequency(Corpus_Hospital_dfm, n = 50)
ggplot(top50_Hospital,
aes(x = reorder(feature,
frequency),
y = frequency)) +
# reorder the words by frequency,
# so that the most frequent words appear at the top
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 50 Words in Hospital Posts",
x = "Words", y = "Frequency") +
theme_minimal()
ggsave("Hospital_Top50.pdf",
plot = last_plot(),
width = 10,
height = 8)
# Install these packages if you haven’t before:
# install.packages("ergm")
library("ergm")
# Set seed to make results reproducible
set.seed(0)
# Load built-in data: marriage ties among Renaissance Florentine families
data(florentine)  # loads flomarriage and flobusiness
flomarriage  # inspect the network object
# Check the vertex (node) attributes
network.vertex.names(flomarriage)
list.vertex.attributes(flomarriage)
# Plot the network
plot(flomarriage,
main = "Florentine Marriage Network",
label = network.vertex.names(flomarriage))
wealth <- flomarriage %v% "wealth"  # get wealth for each family
plot(flomarriage,
vertex.cex = wealth / 25,  # make node size proportional to wealth
label = network.vertex.names(flomarriage),
main = "Node Size = Wealth")
# This is like saying: "Let’s just model tie probability using one parameter: density"
summary(flomarriage ~ edges)  # check observed number of edges
model1 <- ergm(flomarriage ~ edges)  # fit model
summary(model1)
summary(flomarriage ~ edges + triangle)
model2 <- ergm(flomarriage ~ edges + triangle)
summary(model2)
summary(flomarriage ~ edges + nodecov("wealth"))
model3 <- ergm(flomarriage ~ edges + nodecov("wealth"))
summary(model3)
# This checks whether ties are more likely between similarly wealthy families
model4 <- ergm(flomarriage ~ edges + absdiff("wealth"))
summary(model4)
# =============================
# 7. What if we include both wealth and homophily?
# =============================
model5 <- ergm(flomarriage ~ edges + nodecov("wealth") + absdiff("wealth"))
summary(model5)
summary(model2)
# Load necessary libraries
if (!require("quanteda")) install.packages("quanteda")                     # Text processing
if (!require("quanteda.textplots")) install.packages("quanteda.textplots") # Visualizing semantic networks
if (!require("quanteda.textstats")) install.packages("quanteda.textstats") # Co-occurrence and term statistics
if (!require("lexicon")) install.packages("lexicon")                       # For lemmatization
if (!require("ggplot2")) install.packages("ggplot2")                       # For visualization
# Load Congress data
Congress <- read.csv("Congress.csv")
# Explore the dataset
head(Congress)
names(Congress)
# Create corpus and incorporate some document variables
Corpus_Congress <- corpus(Congress$cleanedtext,
docvars = Congress[, c("Page.Name",
"User.Name",
"Facebook.Id",
"Post.Created",
"Comments",
"Shares")])
# Tokenize and clean
Corpus_Congress_tokens <- tokens(Corpus_Congress,
what = "word",
remove_punct = TRUE,      # remove punctuation
remove_symbols = TRUE,    # remove symbols
remove_numbers = TRUE,    # remove numbers
remove_url = TRUE,        # remove URLs
remove_separators = TRUE, # remove separators
split_hyphens = TRUE) %>% # split hyphens
tokens_tolower() %>% # convert to lowercase
tokens_remove(pattern = stopwords("en")) %>% # remove English stopwords
tokens_replace(pattern = lexicon::hash_lemmas$token, # lemmatization
replacement = lexicon::hash_lemmas$lemma) %>%
tokens_remove(c("de", "also", "may", "include", "can", # you can custom words to remove here
"p.m", "a.m", "va", "st", "pm", "datum",
"covid", "vaccination", "vaccinate", "vaccine", "coronavirus",
"2")) %>%
tokens_replace(pattern = "numb", replacement = "number") # you can also replace specific patterns
# Build document-feature-matrix and compute top features
Corpus_Congress_dfm <- Corpus_Congress_tokens %>%
dfm()
topfeatures_Congress <- textstat_frequency(Corpus_Congress_dfm, n = 75)
topfeatures_Congress
topfeatures_Congress_features <- topfeatures_Congress$feature
# Build co-occurrence matrix (FCM: Feature Co-occurrence Matrix)
Corpus_Congress_tokens_fcm <- fcm(Corpus_Congress_tokens,
context = "document",
tri = FALSE) # tri = FALSE means we are not considering tri-grams
# We are not going to visualize the whole network, we are only interested in the top features:
Corpus_Congress_tokens_fcm_trimmed <- fcm_select(Corpus_Congress_tokens_fcm,
pattern = topfeatures_Congress_features)
# Plot and save network
set.seed(123) # for reproducibility
pdf("Congress_Network.pdf", width = 10, height = 10)
textplot_network(Corpus_Congress_tokens_fcm_trimmed,
min_freq = 0.5,
edge_alpha = 0.25,
edge_size = 2)
dev.off()
# Plot top 50 words as a bar chart
top50_Congress <- textstat_frequency(Corpus_Congress_dfm, n = 50)
ggplot(top50_Congress,
aes(x = reorder(feature,
frequency),
y = frequency)) +
# reorder the words by frequency,
# so that the most frequent words appear at the top
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 50 Words in Congress Posts",
x = "Words", y = "Frequency") +
theme_minimal()
# Load Agency data
Agency <- read.csv("Agency.csv")
# Explore the dataset
head(Agency)
names(Agency)
# Create corpus and incorporate some document variables
Corpus_Agency <- corpus(Agency$cleanedtext,
docvars = Agency[, c("Page.Name",
"User.Name",
"Facebook.Id",
"Post.Created",
"Comments",
"Shares")])
# Tokenize and clean
Corpus_Agency_tokens <- tokens(Corpus_Agency,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
split_hyphens = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("en")) %>%
tokens_replace(pattern = lexicon::hash_lemmas$token,
replacement = lexicon::hash_lemmas$lemma) %>%
tokens_remove(c("de", "also", "may", "include", "can",
"p.m", "a.m", "va", "st", "pm", "datum",
"covid", "vaccination", "vaccinate", "vaccine", "coronavirus",
"2")) %>%
tokens_replace(pattern = "numb", replacement = "number")
# Build document-feature-matrix and compute top features
Corpus_Agency_dfm <- Corpus_Agency_tokens %>%
dfm()
topfeatures_Agency <- textstat_frequency(Corpus_Agency_dfm, n = 75)
topfeatures_Agency
topfeatures_Agency_features <- topfeatures_Agency$feature
# Build co-occurrence matrix (FCM)
Corpus_Agency_tokens_fcm <- fcm(Corpus_Agency_tokens,
context = "document",
tri = FALSE)
Corpus_Agency_tokens_fcm_trimmed <- fcm_select(Corpus_Agency_tokens_fcm,
pattern = topfeatures_Agency_features)
# Plot and save network
set.seed(123) # for reproducibility
pdf("Agency_Network.pdf", width = 10, height = 10)
textplot_network(Corpus_Agency_tokens_fcm_trimmed,
min_freq = 0.5,
edge_alpha = 0.25,
edge_size = 2)
dev.off()
# Plot top 50 words as a bar chart
top50_Agency <- textstat_frequency(Corpus_Agency_dfm, n = 50)
ggplot(top50_Agency,
aes(x = reorder(feature,
frequency),
y = frequency)) +
# reorder the words by frequency,
# so that the most frequent words appear at the top
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 50 Words in Agency Posts",
x = "Words", y = "Frequency") +
theme_minimal()
ggsave("Agency_Top50.pdf",
plot = last_plot(),
width = 10,
height = 8)
# Load Hospital data
Hospital <- read.csv("Hospital.csv")
# Explore the dataset
head(Hospital)
names(Hospital)
# Create corpus and incorporate some document variables
Corpus_Hospital <- corpus(Hospital$cleanedtext,
docvars = Hospital[, c("Page.Name",
"User.Name",
"Facebook.Id",
"Post.Created",
"Comments",
"Shares")])
# Tokenize and clean
Corpus_Hospital_tokens <- tokens(Corpus_Hospital,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
split_hyphens = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords("en")) %>%
tokens_replace(pattern = lexicon::hash_lemmas$token,
replacement = lexicon::hash_lemmas$lemma) %>%
tokens_remove(c("de", "also", "may", "include", "can",
"p.m", "a.m", "va", "st", "pm", "datum",
"covid", "vaccination", "vaccinate", "vaccine", "coronavirus",
"2")) %>%
tokens_replace(pattern = "numb", replacement = "number")
# Build document-feature-matrix and compute top features
Corpus_Hospital_dfm <- Corpus_Hospital_tokens %>%
dfm()
topfeatures_Hospital <- textstat_frequency(Corpus_Hospital_dfm, n = 75)
topfeatures_Hospital
topfeatures_Hospital_features <- topfeatures_Hospital$feature
# Build co-occurrence matrix (FCM)
Corpus_Hospital_tokens_fcm <- fcm(Corpus_Hospital_tokens,
context = "document",
tri = FALSE)
Corpus_Hospital_tokens_fcm_trimmed <- fcm_select(Corpus_Hospital_tokens_fcm,
pattern = topfeatures_Hospital_features)
# Plot and save network
set.seed(123) # for reproducibility
pdf("Hospital_Network.pdf", width = 10, height = 10)
textplot_network(Corpus_Hospital_tokens_fcm_trimmed,
min_freq = 0.5,
edge_alpha = 0.25,
edge_size = 2)
dev.off()
# Plot top 50 words as a bar chart
top50_Hospital <- textstat_frequency(Corpus_Hospital_dfm, n = 50)
ggplot(top50_Hospital,
aes(x = reorder(feature,
frequency),
y = frequency)) +
# reorder the words by frequency,
# so that the most frequent words appear at the top
geom_col(fill = "steelblue") +
coord_flip() +
labs(title = "Top 50 Words in Hospital Posts",
x = "Words", y = "Frequency") +
theme_minimal()
ggsave("Hospital_Top50.pdf",
plot = last_plot(),
width = 10,
height = 8)
# ==============================================
# 1. Load necessary packages
# ==============================================
if (!require("igraph")) install.packages("igraph")
# Assign human-readable names to each node
node_names <- c("Alice", "Bob", "Carlos", "Diana", "Eva", "Frank", "Grace", "Hannah",
"Isaac", "Jack", "Kira", "Liam", "Mona", "Nina", "Oscar", "Priya")
# Create a name mapping: 1 = Alice, 2 = Bob, etc.
name_map <- setNames(node_names, 1:16)
# Edge list: connections between individuals
edges <- data.frame(
from = name_map[c(1,1,1,1,2,2,2,3,3,4,3,4,7,7,7,7,1,12,13,13,13,14,14,15)],
to   = name_map[c(2,3,4,5,3,4,5,4,5,5,6,7,8,9,10,11,12,13,14,15,16,15,16,16)]
)
# Take a look at the edge list
print(edges)
# ==============================================
# 3. Create and inspect the graph
# ==============================================
# Create a graph object from the edge list
graph_net <- graph_from_data_frame(edges, directed = FALSE)
# Inspect the graph structure
print(graph_net)
cat("Number of nodes:", vcount(graph_net), "\n")
cat("Number of edges:", ecount(graph_net), "\n")
# ==============================================
# 4. Basic plot of the network
# ==============================================
plot(graph_net,
vertex.label = V(graph_net)$name,
vertex.color = "lightblue",
vertex.size = 20,
edge.arrow.size = 0.5,
main = "Basic Network Visualization")
# ==============================================
# 5. Different layout visualizations
# ==============================================
# Force-directed layout
plot(graph_net, layout = layout_with_fr, main = "Fruchterman-Reingold Layout")
# Tree layout
plot(graph_net, layout = layout_as_tree, main = "Tree Layout")
# Circular layout
plot(graph_net, layout = layout_in_circle, main = "Circular Layout")
# Kamada-Kawai layout
plot(graph_net, layout = layout_with_kk, main = "Kamada-Kawai Layout")
# Create a new edge list with weights (tie strength)
edges_weighted <- data.frame(
from = name_map[c(1,1,1,1,2,2,2,3,3,4,3,4,7,7,7,7,1,12,13,13,13,14,14,15)],
to   = name_map[c(2,3,4,5,3,4,5,4,5,5,6,7,8,9,10,11,12,13,14,15,16,15,16,16)],
weight = c(1,3,1,1,1,1,1,1,3,1,1,1,3,5,2,5,1,5,1,1,1,3,1,1)
)
# Create the weighted graph
graph_weighted <- graph_from_data_frame(edges_weighted, directed = FALSE)
# Assign edge width based on weight
E(graph_weighted)$width <- E(graph_weighted)$weight
# Plot with weights
plot(graph_weighted,
layout = layout_with_fr,
vertex.label = V(graph_weighted)$name,
vertex.color = "lightgreen",
vertex.size = 20,
edge.width = E(graph_weighted)$width,
main = "Network with Weighted Edges")
# ==============================================
# 7. Save the network as an image
# ==============================================
png("network_plot.png", width = 3000, height = 3000, unit = "px", res = 500)
plot(graph_weighted,
layout = layout_with_fr,
vertex.label = V(graph_weighted)$name,
vertex.color = "lightgreen",
vertex.size = 20,
edge.width = E(graph_weighted)$width,
main = "Saved Network Plot")
dev.off()
if (!require("ggraph")) install.packages("ggraph")
if (!require("tidygraph")) install.packages("tidygraph")
# 5. Make a beautiful plot using ggraph and tidygraph
tg <- as_tbl_graph(graph_weighted) %>%
mutate(community = as.factor(group_louvain())) %>% # Community detection for coloring
mutate(size = centrality_degree()) %>% # Calculate node degree for sizing
activate(edges) %>%
mutate(weight = weight)  # ensure 'weight' exists
set.seed(123)  # For reproducibility
ggraph(tg, layout = "fr") +
# layout options: kk, fr, circle, tree, dendrogram
geom_edge_link(aes(width = weight),  # Tie weight visualized as thickness
alpha = 0.9,          # Transparency of edges
color = "gray70") +   # Edge color
geom_node_point(aes(size = size,           # Node size based on degree
color = community),    # Color by community
alpha = 0.9) +             # Transparency of nodes
geom_node_label(
aes(label = name),   # Node labels
size = 3,            # Label size
label.size = 0.3,    # Border size around labels
color = "black",     # Label color
fill = "#F8F8F8",    # Label background color
fontface = "bold",   # Bold labels
alpha = 0.7,         # Transparency of labels
repel = TRUE         # Repel labels to avoid overlap
) +
scale_edge_width(range = c(0.2, 1)) + # Edge width range
scale_size(range = c(4, 10)) +        # Node size range
theme_void() +
labs(edge_width = "Tie Strength",     # legend settings
size = "Node Degree",
color = "Community") +
theme(
legend.position = c(0.2, 0.3),   # (x, y) in [0, 1] relative to plot area
legend.box = "horizontal"        # optional: stack all legends vertically
)
ggsave("network_plot_ggraph.pdf",
width = 10,
height = 7)
