{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "oXr8Wkj7tZvv",
   "metadata": {
    "id": "oXr8Wkj7tZvv"
   },
   "source": [
    "# SICSS-Chicago 2024 Tutorial (Day 2): Computational Visual Analysis\n",
    "### Yingdan Lu (Department of Communication Studies, Northwestern University)\n",
    "\n",
    "In this tutorial, we will be covering the following basics about computational visual analysis:\n",
    "\n",
    "1. Basic image operations\n",
    "2. Basic color features\n",
    "3. Basic facial features (FACE++)\n",
    "4. Basic visual analysis of videos\n",
    "\n",
    "Acknowledgment: Stanford [CS131](http://vision.stanford.edu/teaching/cs131_fall2021/) & [CS230](https://cs230.stanford.edu/) course materials; course materials, paper replication files, Github code from [Yingdan Lu](https://yingdanlu.com/teaching/) from Northwestern University, [Yilang Peng](https://yilangpeng.com/) from University of Georgia, and [Images as Data for Social Science Research](https://codeocean.com/capsule/8598879/tree/v1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awV8faiTtb6A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21020,
     "status": "ok",
     "timestamp": 1689195185353,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "awV8faiTtb6A",
    "outputId": "50d05f89-7157-437c-a4ad-a1d0a49aa5d6"
   },
   "outputs": [],
   "source": [
    "!pip3 install boto3 pyemd opencv-python pillow webcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NQiABkv8rN9k",
   "metadata": {
    "id": "NQiABkv8rN9k"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "import cv2\n",
    "import webcolors\n",
    "import base64\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "## install Athec package\n",
    "athec_path = os.path.expanduser(\"athecmain/\")\n",
    "sys.path.append(athec_path)\n",
    "from athec import misc, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8axbfo7v17z3",
   "metadata": {
    "id": "8axbfo7v17z3"
   },
   "outputs": [],
   "source": [
    "# img_folder_path = '/content/drive/MyDrive/image_as_data/images/'\n",
    "img_folder_path = 'images/'\n",
    "vid_folder_path = 'videos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-senate",
   "metadata": {
    "id": "logical-senate"
   },
   "source": [
    "# BASIC IMAGE OPERATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8ad249-e217-4321-ac86-c765ccb3eb35",
   "metadata": {
    "id": "logical-senate"
   },
   "source": [
    "### Read an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-twins",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 793
    },
    "executionInfo": {
     "elapsed": 2594,
     "status": "ok",
     "timestamp": 1689195189721,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "ancient-twins",
    "outputId": "894cef31-48db-44e2-97c3-7ce56645cfbc"
   },
   "outputs": [],
   "source": [
    "img = Image.open(os.path.join(img_folder_path, 'dog.jpeg'))\n",
    "img = img.convert(\"RGB\") # convert the image to the \"RGB\" mode, regardless of its original mode\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-politics",
   "metadata": {
    "id": "anticipated-politics"
   },
   "source": [
    "You can also use other packages like skimage.io.imread (skimage) or cv2 to read an image, but keep in mind that their ***default color channels can be different*** (e.g., RGB for PIL while BGR for cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-reply",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1689195189722,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "worth-reply",
    "outputId": "aaad96fe-8a24-4167-bd0d-b5608a6e5b5a"
   },
   "outputs": [],
   "source": [
    "# Convert the image object into a NumPy array\n",
    "img_array = np.array(img)\n",
    "img_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-semester",
   "metadata": {
    "id": "humanitarian-semester"
   },
   "source": [
    "The way computers interpret images is by treating them as ***arrays of numerical pixel values***. Each pixel represents a specific color, which is formed by combining three primary colors: red, green, and blue. Each of these colors is assigned a decimal value, ranging from 0 to 255. A value of 0 indicates complete blackness, while 255 represents pure white. Consequently, every color pixel can be described using three numbers, known as RGB decimal values.\n",
    "<div>\n",
    "<img src=\"images/computer_see.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-costume",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1689195189723,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "answering-costume",
    "outputId": "4b6d5b53-efb7-474b-d183-fdd5466db39b"
   },
   "outputs": [],
   "source": [
    "# Print the shape of the Numpy array\n",
    "img_array.shape #height of 876 pixels, width of 1200 pixels, and 3 color channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88afe152-2690-4cb7-8d00-02bd38983538",
   "metadata": {
    "id": "logical-senate"
   },
   "source": [
    "### Basic image manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-tooth",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 1327,
     "status": "ok",
     "timestamp": 1689195191040,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "graphic-tooth",
    "outputId": "9f5f8ffe-dd70-4758-edbe-10fe4c5ca2ba"
   },
   "outputs": [],
   "source": [
    "## Resize the image to its 1/3 size\n",
    "smaller_img = img.resize((int(img.width * 0.3), int(img.height * 0.3)))\n",
    "smaller_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-portfolio",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1689195191041,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "stuffed-portfolio",
    "outputId": "2fdb913b-db5c-41c5-e51c-1b7f9dd01b2b"
   },
   "outputs": [],
   "source": [
    "# Print the shape of the smaller image\n",
    "np.array(smaller_img).shape #height of 262 pixels, width of 360 pixels, and 3 color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-gothic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1689195191049,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "cardiovascular-gothic",
    "outputId": "1c74a329-6948-4821-c5aa-55a4c1177176"
   },
   "outputs": [],
   "source": [
    "# Do a horizontal flip\n",
    "hor_flippedImage = smaller_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "hor_flippedImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-failure",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 59,
     "status": "ok",
     "timestamp": 1689195191050,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "south-failure",
    "outputId": "dc7eea5e-2479-4769-9871-fcfb221cad91"
   },
   "outputs": [],
   "source": [
    "# Do a vertical flip\n",
    "ver_flippedImage = smaller_img.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "ver_flippedImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-constitution",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 247
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1689195191050,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "sufficient-constitution",
    "outputId": "71c33934-3431-4ed4-bc08-56808ec95f9c"
   },
   "outputs": [],
   "source": [
    "#Crop the image\n",
    "#Imput a 4-tuple defining the left, upper, right, and lower pixel coordinate\n",
    "cropped = smaller_img.crop((100,20,280,250))\n",
    "cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-danger",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1689195191051,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "worst-danger",
    "outputId": "ef25c78d-e45f-4005-fcc6-78cd66be9bc5"
   },
   "outputs": [],
   "source": [
    "# Blur an image\n",
    "blurImage = smaller_img.filter(ImageFilter.BLUR)\n",
    "blurImage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-debut",
   "metadata": {
    "id": "israeli-debut"
   },
   "source": [
    "# COLOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-heavy",
   "metadata": {
    "id": "beneficial-heavy"
   },
   "source": [
    "Among all objective features of an image, the color feature is the most widely explored feature in the literature of information science, communication, psychology and advertisement. Color is the result of interaction between physical light in the environment and our visual system, and we can understand the color features of an image using different descriptors.\n",
    "\n",
    "**<span style=\"font-size: larger;\">Color space</span>**, also known as the color model (or color system), is an abstract mathematical model which describes the range of colors as tuples of numbers, typically as 3 or 4 values or color components. A colored image can be described with different color spaces. The RGB color space, for example, uses different combinations of red (R), green (G), and blue (B) to reproduce a broad range of colors. Each pixel uses three numbers that range from 0 to 255 to indicate red, green, and blue, respectively. For example, rgb(255, 0, 0) is displayed as red, because red is set to its highest value (255), and the other two (green and blue) are set to 0. In a grayscale image, each pixel is a single number that represents different shades of gray, typically ranging from 0 (black) to 255 (white)\n",
    "\n",
    "<div>\n",
    "<img src=\"images/color_space.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-shopper",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1689195191051,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "weekly-shopper",
    "outputId": "e9faf392-009a-45a8-91f0-a2a5bf52e2ee"
   },
   "outputs": [],
   "source": [
    "# Convert the image to grayscale\n",
    "img_gray = smaller_img.convert('L')\n",
    "img_gray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-mystery",
   "metadata": {
    "id": "coated-mystery"
   },
   "source": [
    "We can also recolor / change specific color of an image. Let's say we want to change all white color in this image to yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-richards",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1689195191051,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "alone-richards",
    "outputId": "7a2ae1c6-4422-4b6f-8d36-b959e68752e3"
   },
   "outputs": [],
   "source": [
    "smaller_img_yellow = smaller_img.copy() # create a copy of the original image for manipulation\n",
    "d = smaller_img.getdata() # retrieve the pixel data from the smaller_img image\n",
    "\n",
    "new_image = []\n",
    "for item in d:\n",
    "    # change all white (also shades of whites) pixels to yellow\n",
    "    if item[0] in list(range(200, 256)): # Check if the red component (item[0]) of the current pixel value is within the range of 200 to 255.\n",
    "                                         #If yes, it means that the pixel is a shade of white.\n",
    "        new_image.append((255, 224, 100)) #(255, 224, 100) is the RGB value for yellow color\n",
    "    else:\n",
    "        new_image.append(item)\n",
    "# update image data\n",
    "smaller_img_yellow.putdata(new_image)\n",
    "smaller_img_yellow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-restaurant",
   "metadata": {
    "id": "democratic-restaurant"
   },
   "source": [
    "The color space is useful as we can extract specific values (e.g., brightness) from the color space. These specific values are important descriptors of an image. In social science research using images as data, many studies have found that these descriptors can be influencing human perceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-luther",
   "metadata": {
    "id": "educated-luther"
   },
   "source": [
    "### Brightness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-franklin",
   "metadata": {
    "id": "dramatic-franklin"
   },
   "source": [
    "One important color space that we often use is the ***Lab color space***, in which L represents perceptual lightness, a represents the green–red continuum, and b represents the blue–yellow continuum. We can extract the perceptual lightness of pixels from the L space to denote the ***brightness*** of an image.\n",
    "\n",
    "In social science research, the Lab color model is frequently used to quantify skin tone, with L, a, and b indicating skin lightness, redness, and yellowness, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-parameter",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1689195191052,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "middle-parameter",
    "outputId": "870cfbe2-0ed4-4677-f99a-2386e315f533"
   },
   "outputs": [],
   "source": [
    "# Open two images and see how they differ in brightness\n",
    "image1 = Image.open(os.path.join(img_folder_path, 'dog.jpeg'))\n",
    "image2 = Image.open(os.path.join(img_folder_path, 'dog_dark.jpeg'))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(image1)\n",
    "axes[0].axis('off')\n",
    "axes[0].text(0.5, -0.1, \"Image A\", transform=axes[0].transAxes,\n",
    "             fontsize=12, ha='center')\n",
    "axes[1].imshow(image2)\n",
    "axes[1].axis('off')\n",
    "axes[1].text(0.5, -0.1, \"Image B\", transform=axes[1].transAxes,\n",
    "             fontsize=12, ha='center')\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-things",
   "metadata": {
    "id": "deluxe-things"
   },
   "outputs": [],
   "source": [
    "# Write a function to compute the perceptual lightness (brightness)\n",
    "def brightness(image):\n",
    "    img = cv2.imread(image)\n",
    "    img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    #cv2.imread() reads the image in the BGR color format and it needs to be changed to Lab space for computation\n",
    "    results = img_lab[...,0].mean()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-screen",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1689195191053,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "wooden-screen",
    "outputId": "31923acb-3728-4350-cb67-3ec5a050c9e5"
   },
   "outputs": [],
   "source": [
    "print(\"The brightness of image A is: \" + str(brightness(os.path.join(img_folder_path, 'dog.jpeg'))))\n",
    "print(\"The brightness of image B is: \" + str(brightness(os.path.join(img_folder_path, 'dog_dark.jpeg'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-exploration",
   "metadata": {
    "id": "statewide-exploration"
   },
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-father",
   "metadata": {
    "id": "convinced-father"
   },
   "source": [
    "Image entropy is a measure of color complexity and is operationalized as the heterogeneity of pixels. To calculate entropy, you can follow the following steps:\n",
    "1. Convert the color pixels to grayscale.\n",
    "2. Compute the share of pixels with each grayscale color.\n",
    "3. Derive the entropy score across the 256 grayscale colors using Shannon’s entropy metric, with the minimum entropy at 0 (a monochromatic light) and the maximum entropy at 8 (picture with extremely complicated use of color)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-isaac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1689195191402,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "welcome-isaac",
    "outputId": "6b0b8907-fab5-4229-e4bd-c6be40a33a44"
   },
   "outputs": [],
   "source": [
    "# Open two images and see how they differ in entropy\n",
    "image1 = Image.open(os.path.join(img_folder_path, 'dog.jpeg'))\n",
    "image2 = Image.open(os.path.join(img_folder_path, 'dog_black.jpeg'))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes[0].imshow(image1)\n",
    "axes[0].axis('off')\n",
    "axes[0].text(0.5, -0.1, \"Image A\", transform=axes[0].transAxes,\n",
    "             fontsize=12, ha='center')\n",
    "axes[1].imshow(image2)\n",
    "axes[1].axis('off')\n",
    "axes[1].text(0.5, -0.1, \"Image C\", transform=axes[1].transAxes,\n",
    "             fontsize=12, ha='center')\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-halifax",
   "metadata": {
    "id": "realistic-halifax"
   },
   "outputs": [],
   "source": [
    "# Write a function to compute the image entropy\n",
    "def entropy(image):\n",
    "    img = cv2.imread(image)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    signal = gray[0:gray.shape[0],0:gray.shape[1]].flatten()\n",
    "    lensig=signal.size\n",
    "    symset=list(set(signal))\n",
    "    numsym=len(symset)\n",
    "    propab=[np.size(signal[signal==i])/(1.0*lensig) for i in symset]\n",
    "    ent=np.sum([p*np.log2(1.0/p) for p in propab])\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-earthquake",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1689195191613,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "freelance-earthquake",
    "outputId": "98bc0c3f-4f3d-4a83-afa8-8d5f0cb40f40"
   },
   "outputs": [],
   "source": [
    "print(\"The entropy of image A is: \" + str(entropy(os.path.join(img_folder_path, 'dog.jpeg'))))\n",
    "print(\"The entropy of image C is: \" + str(entropy(os.path.join(img_folder_path, 'dog_black.jpeg'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-portfolio",
   "metadata": {
    "id": "iraqi-portfolio"
   },
   "source": [
    "### Specific colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-copper",
   "metadata": {
    "id": "early-copper"
   },
   "source": [
    "Besides color descriptors, different colors may be influencing visual engagement and emotional reactions in different ways. Warm colors like reds and yellows were associated with higher content engagement on social media than cool colors like blues and greens, according to prior research on images.\n",
    "\n",
    "We can use the ***color.attr_color_percentage*** function in the ***Athec*** package (Peng, 2022) to calculate the percentage of eleven basic colors in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-creature",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 155,
     "status": "ok",
     "timestamp": 1689195191765,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "lovely-creature",
    "outputId": "31703844-8d18-46a8-be17-8dfbda4a9820"
   },
   "outputs": [],
   "source": [
    "color.attr_color_percentage(os.path.join(img_folder_path, 'dog.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-judge",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1689195192124,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "legendary-judge",
    "outputId": "bbf4680b-6916-4206-8f7d-10666d7bf301"
   },
   "outputs": [],
   "source": [
    "color.attr_color_percentage(os.path.join(img_folder_path, 'dog_black.jpeg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-burns",
   "metadata": {
    "id": "elementary-burns"
   },
   "source": [
    "### Other color features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-jason",
   "metadata": {
    "id": "blocked-jason"
   },
   "source": [
    "Athec also enables us to compute other color descriptors and features (including some descriptors mentioned above). See more in Peng (2022): https://www.aup-online.com/content/journals/10.5117/CCR2022.1.009.PENG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-formation",
   "metadata": {
    "id": "mental-formation"
   },
   "outputs": [],
   "source": [
    "# Write a function to compute low-level features of an image\n",
    "def color_dict(img_name):\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate summary statistics of RGB channels.\n",
    "    \"\"\"\n",
    "    result_1 = color.attr_RGB(img_name, return_full = False)\n",
    "    result.update(result_1)\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate summary statistics of HSV channels.\n",
    "    \"\"\"\n",
    "    result_2 = color.attr_HSV(img_name, return_full = False)\n",
    "    result.update(result_2)\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate summary statistics of HSL channels.\n",
    "    \"\"\"\n",
    "    result_3 = color.attr_HSL(img_name,return_full = False)\n",
    "    result.update(result_3)\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate summary statistics of XYZ channels\n",
    "    \"\"\"\n",
    "    result_4 = color.attr_XYZ(img_name,return_full = False)\n",
    "    result.update(result_4)\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate summary statistics of Lab channels\n",
    "    \"\"\"\n",
    "    result_5 = color.attr_Lab(img_name,return_full = False)\n",
    "    result.update(result_5)\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate summary statistics of grayscale channel\n",
    "    \"\"\"\n",
    "    result_6 = color.attr_grayscale(img_name,return_full = False)\n",
    "    result.update(result_6)\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate contrast based on a range that covers a certain percentage of the brightness histogram.\n",
    "    Return the range and its lower and upper limits.\n",
    "    save_path (optional, default None): str. If provided, a visualization will be saved to this location.\n",
    "    threshold (optional, default 0.9): float. The percentage the range covers.\n",
    "    \"\"\"\n",
    "    result_7 = color.attr_contrast_range(img_name,threshold = 0.90)\n",
    "    result.update(result_7)\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate contrast based on peak detection on the brightness histogram.\n",
    "    \"\"\"\n",
    "    result_8 = color.attr_contrast_peak(img_name,\n",
    "                                  savgol_filter_window_length = 51,\n",
    "                                  savgol_filter_polyorder = 5,\n",
    "                                  savgol_filter_mode = \"constant\",\n",
    "                                  argrelmax_order = 20)\n",
    "    result.update(result_8)\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate colorfulness based on the formula in Hasler and Suesstrunk (2003)\n",
    "    \"\"\"\n",
    "    result_9 = color.attr_colorful(img_name)\n",
    "    result.update(result_9)\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate colorfulness based on the distance between two color distributions (Datta et al., 2006)\n",
    "    \"\"\"\n",
    "    result_10 = color.attr_colorful_emd(img_name)\n",
    "    result.update(result_10)\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate percentages of eleven specific colors and color variety measures based on color percentages (excluding black, white, and gray).\n",
    "    color_dict (optional, default None): if not provided, the function will automatically import the dictionary using colordict.color_dict().\n",
    "    save_path (optional, default None): str. If provided, a visualization will be saved to this location.\n",
    "    \"\"\"\n",
    "    result_11 = color.attr_color_percentage(img_name)\n",
    "    result.update(result_11)\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate color variety based on hue count formula in Ke et al. (2006).\n",
    "    save_path (optional, default None): str. If provided, a visualization will be saved to this location.\n",
    "    saturation_low (optional, default 0.2): float. The lower limit for saturation. Pixels with saturation below the limit are discarded.\n",
    "    value_low (optional, default 0.15): float. The lower limit for value. Pixels with value below the limit are discarded.\n",
    "    value_high (optional, default 0.95): float. The upper limit for value. Pixels with value above the limit are discarded.\n",
    "    hue_count_alpha (optional, default 0.05): float. Alpha in the hue count formula.\n",
    "    \"\"\"\n",
    "    result_12 = color.attr_hue_count(img_name,\n",
    "                              saturation_low = 0.2,\n",
    "                              value_low = 0.15,\n",
    "                              value_high = 0.95,\n",
    "                              hue_count_alpha = 0.05)\n",
    "    result.update(result_12)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-glass",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6043,
     "status": "ok",
     "timestamp": 1689195198164,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "acquired-glass",
    "outputId": "bf8d0ba6-87c2-4507-9e1d-05832de9b361"
   },
   "outputs": [],
   "source": [
    "color_dict(os.path.join(img_folder_path, 'dog_black.jpeg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-technician",
   "metadata": {
    "id": "unable-technician"
   },
   "source": [
    "# FACES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-pierce",
   "metadata": {
    "id": "advance-pierce"
   },
   "source": [
    "Facial features can be easily computed using off-the-shelf APIs. Among the popular choices is the Face++ API.\n",
    "\n",
    "**Face++ Image Requirements:**\n",
    "*   Format : JPG (JPEG), PNG\n",
    "*   Size : between 48x48 and 4096x4096 (pixels)\n",
    "*   File size : no larger than 2MB\n",
    "*   Minimal size of face : the bounding box of a detected face is a square. The minimal side length of a square should be no less than 1/48 of the short side of image, and no less than 48 pixels. For example if the size of image is 4096 * 3200px, the minimal size of face should be 66 * 66px.\n",
    "  \n",
    "*   Face++ API document: https://console.faceplusplus.com/documents/5679127\n",
    "*   Face++ API can conduct face detection in many aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-borough",
   "metadata": {
    "id": "animated-borough"
   },
   "outputs": [],
   "source": [
    "# Prepare the credentials for Face⁺⁺\n",
    "# USE YOUR CREDENTIALS\n",
    "http_url = 'https://api-us.faceplusplus.com/facepp/v3/detect'\n",
    "key = \"4GpVFFtf-ywJ5Gg1mXo-TKD2V6ywQlEg\"\n",
    "secret = \"xmY1Vm5f4jURK2DCCpRHy4Q0gwVgkrca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-herald",
   "metadata": {
    "id": "handmade-herald"
   },
   "outputs": [],
   "source": [
    "# Write a function to detect the number of faces\n",
    "def face_extract(image):\n",
    "    face_count = 0\n",
    "    # Read the image and convert it to base64 format (transmit image data reliably and conveniently)\n",
    "    files = open(image, \"rb\")\n",
    "    encoded_string = base64.b64encode(files.read())\n",
    "\n",
    "    # get response from Face++ API\n",
    "    response = requests.post(http_url, data={\"api_key\":key,\n",
    "                                             \"api_secret\": secret,\n",
    "                                             \"return_attributes\": \"gender,smiling,emotion\",\n",
    "                                             'image_base64': encoded_string,\n",
    "                                             'return_landmark': 1})\n",
    "    js = json.loads(response.text)\n",
    "\n",
    "    # results (number of faces)\n",
    "    face_count = len(js[\"faces\"])\n",
    "    print(face_count, \"faces were detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-remark",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 726,
     "output_embedded_package_id": "1xTqiKwQSKXkOrJ6m2esiaMNBqyfNzYlU"
    },
    "executionInfo": {
     "elapsed": 7901,
     "status": "ok",
     "timestamp": 1689195219284,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "spanish-remark",
    "outputId": "f1c8616d-a541-4c31-a557-097e74e7618f"
   },
   "outputs": [],
   "source": [
    "Image.open(os.path.join(img_folder_path, 'presidents.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-creativity",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2111,
     "status": "ok",
     "timestamp": 1689195239859,
     "user": {
      "displayName": "Yingdan Lu",
      "userId": "02994884030867932939"
     },
     "user_tz": 420
    },
    "id": "accepted-creativity",
    "outputId": "84effdd7-ec16-4a94-ead2-61abf917e171"
   },
   "outputs": [],
   "source": [
    "face_extract(os.path.join(img_folder_path, 'presidents.jpeg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-devon",
   "metadata": {
    "id": "subtle-devon"
   },
   "source": [
    "As shown above, besides counting faces you can also calculate more facial attributes of each face using face++. For example, we can know how many faces are male faces, smiling faces, angry faces, etc. You can find more details via this link: https://console.faceplusplus.com/documents/5679127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XFWAaFCm8ccs",
   "metadata": {
    "id": "XFWAaFCm8ccs"
   },
   "outputs": [],
   "source": [
    "# Write a function to detect the number of faces\n",
    "def face_extract_more(image):\n",
    "    face_count = 0\n",
    "    # Read the image and convert it to base64 format (transmit image data reliably and conveniently)\n",
    "    files = open(image, \"rb\")\n",
    "    encoded_string = base64.b64encode(files.read())\n",
    "\n",
    "    # get response from Face++ API\n",
    "    response = requests.post(http_url, data={\"api_key\":key,\n",
    "                                             \"api_secret\": secret,\n",
    "                                             \"return_attributes\": \"gender,smiling,emotion,age\",\n",
    "                                             'image_base64': encoded_string,\n",
    "                                             'return_landmark': 1})\n",
    "    js = json.loads(response.text)\n",
    "\n",
    "    face_count = len(js[\"faces\"])\n",
    "    print(\"Total faces: \", face_count)\n",
    "    print(\"=================\")\n",
    "\n",
    "    for faceid in range(0, face_count):\n",
    "       if faceid > 4: continue # Free version of the API only analyzes first five faces\n",
    "       print(\"face id: \", faceid)\n",
    "       this_face = js[\"faces\"][faceid]\n",
    "       width = this_face[\"face_rectangle\"][\"width\"]; height = this_face[\"face_rectangle\"][\"height\"]\n",
    "       top = this_face[\"face_rectangle\"][\"top\"]; left = this_face[\"face_rectangle\"][\"left\"]\n",
    "       size = width * height\n",
    "       print(\"face size: \",size)\n",
    "       gender = this_face[\"attributes\"][\"gender\"][\"value\"]\n",
    "       age = this_face[\"attributes\"][\"age\"][\"value\"]\n",
    "       emotion_info = this_face[\"attributes\"][\"emotion\"]\n",
    "       # Notes on emotion outputs: The value of each field is a floating-point number with 3 decimal places between [0,100].\n",
    "       # Bigger value of a field indicates greater confidence of the emotion which the field represents. The sum of all values is 100.\n",
    "       print(\"gender: \",gender)\n",
    "       print(\"age: \",age)\n",
    "       print(\"emotion: \",emotion_info)\n",
    "       print(\"=================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KOHTJWs19DPc",
   "metadata": {
    "id": "KOHTJWs19DPc"
   },
   "outputs": [],
   "source": [
    "face_extract_more(os.path.join(img_folder_path, 'presidents.jpeg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-leave",
   "metadata": {
    "id": "nervous-leave"
   },
   "source": [
    "\n",
    "Besides the above attributes, Face++ API also allows us to detect\n",
    "\n",
    "1. Beauty: Beauty score for male and female. The value is a floating-point number with 3 decimal places betwee [0,100]. Higher beauty score indicates the detected faces is more beautiful.\n",
    "\n",
    "\n",
    "2. Headpose: 3D head pose analysis result, including pitch_angle, roll_angle, and yaw_angle. The value of each object is a floating-point number with 6 decimal places between [-180, 180]\n",
    "\n",
    "\n",
    "3. Blur: face blur condition with a floating-point number with 3 decimal places between [0,100]\n",
    "\n",
    "\n",
    "4. Eyestatus: including left eye status, right eye status, occlusion of eye, whether or not wearing a dark glass, whether or not the eye is open, etc\n",
    "\n",
    "\n",
    "5. Eyegaze: eye center location and eye gaze direction\n",
    "\n",
    "\n",
    "There are also other APIs or pretrained models that can be used for facial recognition and facial attribute analysis, e.g., DeepFace (see: https://github.com/serengil/deepface), face_recognition package (see: https://face-recognition.readthedocs.io/en/latest/readme.html)\n",
    "\n",
    "One thing to keep in mind is to always **<span style=\"font-size: larger;\">VALIDATE</span>** your results when using these off-the-shelf tools. See one problematic example as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-affiliation",
   "metadata": {
    "id": "changing-affiliation"
   },
   "outputs": [],
   "source": [
    "print(face_extract_more(os.path.join(img_folder_path, 'dog.jpeg'))) ## Should be 0 but Face++ tells you there is A HUMAN FACE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ffdc2f-48e5-4f24-9f19-d552017b5620",
   "metadata": {},
   "source": [
    "# BASIC VISUAL ANALYSIS OF VIDEOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32cfd97-5b4b-416e-a4cf-4634ef6b2628",
   "metadata": {},
   "source": [
    "Now that you already know how to work on static images, let's explore how to analyze visuals in video data. Visually, a video can be seen as a sequence of static image frames. To examine visual features from a video, it is important to learn basic operations like loading videos, extracting frames, and applying computer vision approaches to analyze these visual frames.\n",
    "\n",
    "<div>\n",
    "<img src=\"images/videos.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb241e5-f642-49f6-b6ce-15101de42749",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load a video\n",
    "def load_video(video):\n",
    "    cap = cv2.VideoCapture(video) # VideoCapture object to access video frames.\n",
    "    # Check if the video file is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return None\n",
    "    return cap\n",
    "\n",
    "video_path = 'videos/example.mp4'\n",
    "cap = load_video(video_path)\n",
    "cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a39c4-1506-4d07-911c-27bd3dc860bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract frames from the video\n",
    "def extract_frames(cap):\n",
    "    frames = [] # Initialize an empty list to store frames\n",
    "    count = 0 # Frame counter\n",
    "    while cap.isOpened():\n",
    "        # Read a frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break # Exit the loop if the frame loading fails\n",
    "        frames.append(frame)\n",
    "        count += 1\n",
    "    cap.release()\n",
    "    print(\"Total frames: \", count)\n",
    "    return frames\n",
    "\n",
    "# Extract frames\n",
    "video_frames = extract_frames(cap)\n",
    "\n",
    "# Display the first frame\n",
    "plt.imshow(cv2.cvtColor(video_frames[0], cv2.COLOR_BGR2RGB))\n",
    "plt.title('First Frame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fcfb94-d9f5-46b1-8975-c7fb891c79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert frames to grayscale\n",
    "\n",
    "def convert_to_grayscale(frames):\n",
    "    gray_frames = [cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) for frame in frames]  # Convert each frame to grayscale\n",
    "    return gray_frames\n",
    "\n",
    "gray_frames = convert_to_grayscale(video_frames)\n",
    "\n",
    "# Display the first grayscale frame\n",
    "plt.imshow(gray_frames[0], cmap='gray')  # Display the first grayscale frame with gray colormap\n",
    "plt.title('First Grayscale Frame')  # Set the title of the plot\n",
    "plt.show()  # Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d1ad8-a5b9-401c-8d4a-66f5be661ac1",
   "metadata": {},
   "source": [
    "You can also use other computer vision techniques that we mentioned above or others to analyze visual frames from videos and derive video-level features. \n",
    "\n",
    "Besides visuals, you can also analyze textual and auditory information from videos. For example, you can use the Librosa package (https://librosa.org/doc/latest/index.html) to analyze audio features of a video. LLM-based tools like Gemini (see: https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/video-understanding for video understanding) also enable multimodal analyses that consider text, visuals, and audio of videos."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
