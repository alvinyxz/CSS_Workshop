# Remove specific n-grams
toks <- tokens_remove(toks, pattern = phrase(c("united states",
"applause"))) # what else?
# Initial DFM
fullcorpus.dfm <- dfm(toks,
tolower = TRUE,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE)
# Remove stopwords + additional common irrelevant terms
custom_stopwords <- c(stopwords("en",
source = "stopwords-iso"),
"also", "one", "two", "three", "can") # what else?
fullcorpus.dfm <- dfm_remove(fullcorpus.dfm,
custom_stopwords)
topfeatures(fullcorpus.dfm, n = 300)
# Look at the dfm and see what prevalent terms you should remove
# Lemmatization (instead of stemming)
lemmas <- unique(tolower(unlist(toks)))
lemmas <- data.table(words = lemmas,
lemmas = lemmatize_words(lemmas))
fullcorpus.dfm <- dfm_replace(fullcorpus.dfm,
pattern = lemmas$words,
replacement = lemmas$lemmas,
case_insensitive = TRUE)
# Trim rare/common words
# Think about what we mean by topics:
# If it is too common, it is not useful
# if it is too rare, it is not useful either
fullcorpus_trimed <- dfm_trim(fullcorpus.dfm,
min_docfreq = 0.01,
max_docfreq = 0.99,
docfreq_type = "prop")
# Convert for STM
fullcorpus_trimed_stm <- convert(fullcorpus_trimed,
to = "stm")
# Search for optimal K
set.seed(12345)
k_search <- searchK(documents = fullcorpus_trimed_stm$documents,
vocab = fullcorpus_trimed_stm$vocab,
data = fullcorpus_trimed_stm$meta,
prevalence = ~ Party + s(year),
K = seq(5, 40, 5),
max.em.its = 75,
cores = 1)
# Advanced STM analysis of presidential_speeches.csv
# Builds upon STM_1.R and incorporates additional preprocessing and modeling features
# Load necessary libraries
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("quanteda")) install.packages("quanteda")
if (!require("stm")) install.packages("stm")
if (!require("textstem")) install.packages("textstem")
if (!require("data.table")) install.packages("data.table")
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("patchwork")) install.packages("patchwork")
# Load the data
dta <- read.csv("presidential_speeches.csv")
# Preprocess metadata
dta <- dta %>%
filter(!is.na(speech)) %>% # delete empty speeches
mutate(Year = as.numeric(format(as.Date(date, format="%m/%d/%Y"), "%Y")),
President = factor(President),
Party = factor(Party))
# Text size summary
sum(sapply(strsplit(dta$speech, " "), length))  # total words
# Construct corpus with document-level variables
fullcorpus <- corpus(dta$speech,
docvars = dta[, c("President",
"Party",
"Year")]) # document-level variables to model later
# Tokenization
# use "quanteda::" to avoid conflicts with other packages
toks <- quanteda::tokens(fullcorpus,
what = "word",
remove_punct = TRUE,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_url = TRUE,
remove_separators = TRUE,
split_hyphens = FALSE)
# Convert British to American English, not needed for this dataset but very commonly used
if (!require("devtools")) install.packages("devtools")
if (!require("quanteda.dictionaries")) install.packages("kbenoit/quanteda.dictionaries")
toks <- tokens_lookup(toks,
dictionary = data_dictionary_uk2us,
exclusive = FALSE,
capkeys = FALSE)
# Remove specific n-grams
toks <- tokens_remove(toks, pattern = phrase(c("united states",
"applause"))) # what else?
# Initial DFM
fullcorpus.dfm <- dfm(toks,
tolower = TRUE,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE)
# Remove stopwords + additional common irrelevant terms
custom_stopwords <- c(stopwords("en",
source = "stopwords-iso"),
"also", "one", "two", "three", "can") # what else?
fullcorpus.dfm <- dfm_remove(fullcorpus.dfm,
custom_stopwords)
topfeatures(fullcorpus.dfm, n = 300)
# Look at the dfm and see what prevalent terms you should remove
# Lemmatization (instead of stemming)
lemmas <- unique(tolower(unlist(toks)))
lemmas <- data.table(words = lemmas,
lemmas = lemmatize_words(lemmas))
fullcorpus.dfm <- dfm_replace(fullcorpus.dfm,
pattern = lemmas$words,
replacement = lemmas$lemmas,
case_insensitive = TRUE)
# Trim rare/common words
# Think about what we mean by topics:
# If it is too common, it is not useful
# if it is too rare, it is not useful either
fullcorpus_trimed <- dfm_trim(fullcorpus.dfm,
min_docfreq = 0.01,
max_docfreq = 0.99,
docfreq_type = "prop")
# Convert for STM
fullcorpus_trimed_stm <- convert(fullcorpus_trimed,
to = "stm")
# Search for optimal K
set.seed(12345)
k_search <- searchK(documents = fullcorpus_trimed_stm$documents,
vocab = fullcorpus_trimed_stm$vocab,
data = fullcorpus_trimed_stm$meta,
prevalence = ~ Party + s(year),
K = seq(5, 40, 5),
max.em.its = 75,
cores = 1)
k_search <- searchK(documents = fullcorpus_trimed_stm$documents,
vocab = fullcorpus_trimed_stm$vocab,
data = fullcorpus_trimed_stm$meta,
prevalence = ~ Party + s(Year),
K = seq(5, 40, 5),
max.em.its = 75,
cores = 1)
# Search for optimal K
set.seed(12345)
k_search <- searchK(documents = fullcorpus_trimed_stm$documents,
vocab = fullcorpus_trimed_stm$vocab,
data = fullcorpus_trimed_stm$meta,
prevalence = ~ Party + s(Year), # theoretically, what variables will influence your topic prevalence?
K = seq(5, 20, 5), # search for K (the number of topics) from 5 to 40, with increments of 5
max.em.its = 75, # maximum number of iterations to find the best model for a specific K
cores = 1) # How many PC/Mac cores you want to use for parallel processing
# Plot coherence vs. exclusivity
coh_excl_plot <- ggplot(data.frame(K = k_search$results$K,
Coherence = k_search$results$semcoh,
Exclusivity = k_search$results$exclus),
aes(x = K)) +
geom_line(aes(y = Coherence), color = "steelblue") +
geom_line(aes(y = Exclusivity), color = "darkorange") +
labs(title = "Model Diagnostics",
x = "Number of Topics (K)")
print(coh_excl_plot)
# Plot coherence vs. exclusivity
coh_excl_plot <- ggplot(data.frame(K = k_search$results$K,
Coherence = k_search$results$semcoh,
Exclusivity = k_search$results$exclus),
aes(x = K)) +
geom_line(aes(y = Coherence), color = "steelblue") +
geom_line(aes(y = Exclusivity), color = "darkorange") +
labs(title = "Model Diagnostics",
x = "Number of Topics (K)")
# Plot coherence vs. exclusivity
ggplot(data.frame(K = k_search$results$K,
Coherence = k_search$results$semcoh,
Exclusivity = k_search$results$exclus),
aes(x = K)) +
geom_line(aes(y = Coherence), color = "steelblue") +
geom_line(aes(y = Exclusivity), color = "darkorange") +
labs(title = "Model Diagnostics",
x = "Number of Topics (K)")
# Plot coherence vs. exclusivity
diagnostics_df <- data.frame(
K = k_search$results$K,
Coherence = k_search$results$semcoh,
Exclusivity = k_search$results$exclus
)
ggplot(diagnostics_df, aes(x = K)) +
geom_line(aes(y = Coherence), color = "steelblue") +
geom_line(aes(y = Exclusivity), color = "darkorange") +
labs(
title = "Model Diagnostics",
x = "Number of Topics (K)",
y = "Score"
) +
theme_minimal()
diagnostics_df
# Plot coherence vs. exclusivity
diagnostics_df <- data.frame(
K = k_search$results$K,
Coherence = k_search$results$semcoh,
Exclusivity = k_search$results$exclus
)
View(diagnostics_df)
k_search$results$K
# Plot coherence vs. exclusivity
diagnostics_df <- data.frame(
K = unlist(k_search$results$K),
Coherence = unlist(k_search$results$semcoh),
Exclusivity = unlist(k_search$results$exclus)
)
ggplot(diagnostics_df, aes(x = K)) +
geom_line(aes(y = Coherence), color = "steelblue") +
geom_line(aes(y = Exclusivity), color = "darkorange") +
labs(
title = "Model Diagnostics",
x = "Number of Topics (K)",
y = "Score"
) +
theme_minimal()
# Fit final model with selected K
set.seed(12345)
# Fit final model with selected K
# Let's say you think K=10 is the best
set.seed(12345)
mod <- stm(documents = fullcorpus_trimed_stm$documents,
vocab = fullcorpus_trimed_stm$vocab,
data = fullcorpus_trimed_stm$meta,
prevalence = ~ Party + s(year),
K = 10)
mod <- stm(documents = fullcorpus_trimed_stm$documents,
vocab = fullcorpus_trimed_stm$vocab,
data = fullcorpus_trimed_stm$meta,
prevalence = ~ Party + s(Year),
K = 10)
# Label topics
labelTopics(mod)
# Estimate effects
prep <- estimateEffect(1:10 ~ s(Year),
mod,
meta = fullcorpus_trimed_stm$meta)
# Plot topic trends over time
plot.estimateEffect(prep,
covariate = "Year",
mod = mod,
method = "continuous",
topics = 1:10,
xlab = "Year",
ylab = "Topic Prevalence",
main = "Topic Trends Over Time")
# Plot topic trends over time
plot.estimateEffect(prep,
covariate = "Year",
method = "continuous",
topics = 1:10,
model = mod,
xlab = "Year",
ylab = "Topic Prevalence",
main = "Topic Trends Over Time")
# Plot topic trends over time
plot.estimateEffect(prep,
covariate = "Year",
method = "continuous",
topics = 1:10, # or you can specific one specific topic
model = mod,
xlab = "Year",
ylab = "Topic Prevalence",
main = "Topic Trends Over Time")
# Save model
saveRDS(mod, "stm_presidential_speeches.rds")
# Step 1: Install necessary packages if
if (!require("text2vec")) install.packages("text2vec")
if (!require("devtools")) install.packages("devtools")
if (!require("wordVectors")) devtools::install_github("bmschmidt/wordVectors")
if (!require("tidyverse")) install.packages("tidyverse")
# Step 3: Load pre-trained Word2Vec model into R
model <- read.vectors("GoogleNews-vectors-negative300-SLIM.bin", binary = TRUE)
if (!require("devtools")) install.packages("devtools")
if (!require("wordVectors")) devtools::install_github("bmschmidt/wordVectors")
# Step 1: Install necessary packages if
if (!require("text2vec")) install.packages("text2vec")
if (!require("devtools")) install.packages("devtools")
if (!require("wordVectors")) devtools::install_github("bmschmidt/wordVectors")
if (!require("tidyverse")) install.packages("tidyverse")
# Step 3: Load pre-trained Word2Vec model into R
model <- read.vectors("GoogleNews-vectors-negative300-SLIM.bin", binary = TRUE)
# ======================================
# 1. Install and load required packages
# ======================================
install.packages("text2vec")
# ======================================
# 1. Install and load required packages
# ======================================
if (!require("text2vec")) install.packages("text2vec")
if (!require("tidyverse")) install.packages("tidyverse")
# ======================================
# 2. Sample corpus (use your own file here)
# ======================================
text <- c("The cat sits on the mat",
"The dog lies on the rug",
"Dogs and cats are great")
# ======================================
# 3. Create tokens
# ======================================
tokens <- space_tokenizer(tolower(text))
# ======================================
# 4. Create vocabulary and document-term matrix
# ======================================
it <- itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it)
vectorizer <- vocab_vectorizer(vocab)
dtm <- create_tcm(it, vectorizer, skip_grams_window = 5L)
# ======================================
# 5. Fit GloVe model
# ======================================
glove_model <- GlobalVectors$new(rank = 50, x_max = 10)
glove_model
word_vectors <- glove_model$fit_transform(dtm, n_iter = 20)
# Add context vectors
context_vectors <- glove_model$components
word_vectors_final <- word_vectors + t(context_vectors)
# ======================================
# 6. Explore word embeddings
# ======================================
# Cosine similarity between "cat" and others
cat_vector <- word_vectors_final["cat", , drop = FALSE]
cos_sim <- sim2(x = word_vectors_final, y = cat_vector, method = "cosine", norm = "l2")
cat_vector
# Top 5 similar words
head(sort(cos_sim[,1], decreasing = TRUE), 5)
# ====================================================
# Step 1: Install required packages
# ====================================================
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("text2vec")) install.packages("text2vec")
if (!require("Rtsne")) install.packages("Rtsne")
if (!require("ggplot2")) install.packages("ggplot2")
# ====================================================
# Step 3: Load pre-trained GloVe vectors into R
# ====================================================
# Load the GloVe 100-dimensional embedding
glove_embeddings <- read_delim("glove.6B.300d.txt",
delim = " ", quote = "", col_names = FALSE)
# The first column is the word, the rest are vector values
word_vectors <- as.matrix(glove_embeddings[, -1])
rownames(word_vectors) <- glove_embeddings[[1]]
# ====================================================
# Step 4: Explore the loaded model
# ====================================================
# View some metadata
print(dim(word_vectors))        # How many words and dimensions
print(head(rownames(word_vectors)))  # Example words
# You have 400000 words, each word is represented by a 300-dimensional vector (i.e., 300 numbers)
print(head(rownames(word_vectors)), 10)  # Example words
rownames(word_vectors)
# You have 400000 words, each word is represented by a 300-dimensional vector (i.e., 300 numbers)
head(rownames(word_vectors))  # Example words
# You have 400000 words, each word is represented by a 300-dimensional vector (i.e., 300 numbers)
head(rownames(word_vectors), n=20)  # Example words
# You have 400000 words, each word is represented by a 300-dimensional vector (i.e., 300 numbers)
sample(rownames(word_vectors), n=20)  # Example words
sample_words <- sample(rownames(word_vectors), 20)
# View a random sample of 20 words in the embedding
sample(rownames(word_vectors), 20)
# View a random sample of 20 words in the embedding
sample(rownames(word_vectors), 50)
# ====================================================
# Step 5: Find similar words
# ====================================================
# Example: words similar to "king"
king_vec <- word_vectors["king", , drop = FALSE]
similarities <- sim2(word_vectors, king_vec, method = "cosine", norm = "l2")
head(sort(similarities[,1], decreasing = TRUE), 10)
# ====================================================
# Step 5: Find similar words
# ====================================================
# Example: words similar to "king"
king_vec <- word_vectors["king", , drop = FALSE]
# Compute "King"'s cosine similarity with other words
similar_words <- sim2(word_vectors, king_vec, method = "cosine", norm = "l2")
# Sort by similarity and show top 10 similar words
head(sort(similarities[,1], decreasing = TRUE), 10)
# ====================================================
# Step 6: Compute analogies
# ====================================================
# Word algebra: king - man + woman ≈ queen
analogy_vec <- word_vectors["king", ] - word_vectors["man", ] + word_vectors["woman", ]
analogy_result <- sim2(word_vectors, matrix(analogy_vec, nrow = 1), method = "cosine", norm = "l2")
head(sort(analogy_result[,1], decreasing = TRUE), 10)
# ====================================================
# Step 6: Compute analogies
# ====================================================
# Word algebra: king - man + woman ≈ queen
analogy_vec <- word_vectors["king", ] - word_vectors["man", ] + word_vectors["woman", ]
analogy_result <- sim2(word_vectors, matrix(analogy_vec, nrow = 1), method = "cosine", norm = "l2")
head(sort(analogy_result[,1], decreasing = TRUE), 10)
# More analogies:
analogy_vec <- word_vectors["guy", ] - word_vectors["man", ] + word_vectors["woman", ]
print(head(sort(sim2(word_vectors, matrix(analogy_vec, nrow = 1), method = "cosine")[,1], decreasing = TRUE), 10))
analogy_vec <- word_vectors["china", ] - word_vectors["beijing", ] + word_vectors["tokyo", ]
print(head(sort(sim2(word_vectors, matrix(analogy_vec, nrow = 1), method = "cosine")[,1], decreasing = TRUE), 10))
analogy_vec <- word_vectors["tokyo", ] - word_vectors["japan", ] + word_vectors["germany", ]
print(head(sort(sim2(word_vectors, matrix(analogy_vec, nrow = 1), method = "cosine")[,1], decreasing = TRUE), 10))
# ====================================================
# Step 7: Compute similarity between two words
# ====================================================
sim2(word_vectors["beijing", , drop = FALSE],
word_vectors["china", , drop = FALSE],
method = "cosine")
sim2(word_vectors["beijing", , drop = FALSE],
word_vectors["chair", , drop = FALSE],
method = "cosine")
# More analogies (some of them might not be perfect)
analogy_vec <- word_vectors["guy", ] - word_vectors["man", ] + word_vectors["woman", ]
print(head(sort(sim2(word_vectors, matrix(analogy_vec, nrow = 1), method = "cosine")[,1], decreasing = TRUE), 10))
analogy_vec <- word_vectors["china", ] - word_vectors["beijing", ] + word_vectors["tokyo", ]
print(head(sort(sim2(word_vectors, matrix(analogy_vec, nrow = 1), method = "cosine")[,1], decreasing = TRUE), 10))
analogy_vec <- word_vectors["tokyo", ] - word_vectors["japan", ] + word_vectors["germany", ]
print(head(sort(sim2(word_vectors, matrix(analogy_vec, nrow = 1), method = "cosine")[,1], decreasing = TRUE), 10))
sim2(word_vectors["beijing", , drop = FALSE],
word_vectors["china", , drop = FALSE],
method = "cosine")
sim2(word_vectors["beijing", , drop = FALSE],
word_vectors["chair", , drop = FALSE],
method = "cosine")
# Pick some words to visualize
words_to_plot <- c("king", "queen", "man", "woman",
"dog", "cat", "house", "apartment",
"undergraduate", "graduate")
# Extract their vectors
word_matrix <- word_vectors[words_to_plot, ]
# Run t-SNE (dimensionality reduction to 2D)
tsne_result <- Rtsne(word_matrix, dims = 2, perplexity = 2)
# Create data frame for plotting
tsne_df <- data.frame(x = tsne_result$Y[,1],
y = tsne_result$Y[,2],
word = words_to_plot)
tsne_result
word_matrix
# You can print out the matrix to see the vectors
print(word_matrix)
# Run t-SNE (dimensionality reduction to 2D)
tsne_result <- Rtsne(word_matrix, dims = 2, perplexity = 2)
# Create data frame for plotting
tsne_df <- data.frame(x = tsne_result$Y[,1],
y = tsne_result$Y[,2],
word = words_to_plot)
# Plot using ggplot2
ggplot(tsne_df, aes(x = x, y = y, label = word)) +
geom_text(size = 5) +
ggtitle("t-SNE Visualization of Word Embeddings") +
theme_minimal()
# Let's simulate our own mini "corpus" (i.e., collection of text)
# We'll train a GloVe model from scratch on this toy dataset
text <- c("The cat sits on the mat",
"The dog lies on the rug",
"Dogs and cats are great")
# Convert to lowercase and split each sentence into words (tokens)
tokens <- text %>%
tolower() %>%
space_tokenizer()
tokens
# Each sentence is now a list of individual words
print(tokens)
# Turn the tokens into an iterator — a stream of tokenized docs
it <- itoken(tokens, progressbar = FALSE)
it
# Build a vocabulary: just a list of all words in the corpus
vocab <- create_vocabulary(it)
vocab
# Create a vectorizer using the vocabulary
vectorizer <- vocab_vectorizer(vocab)
vectorizer
# Create a term co-occurrence matrix (TCM)
# This matrix captures how often each word appears near others
# The window size determines the range of "context" around each word
tcm <- create_tcm(it, vectorizer, skip_grams_window = 5L)
tcm
# Fit the model to the term co-occurrence matrix
# n_iter controls how many training steps to run (more = better but slower)
word_vectors <- glove$fit_transform(tcm, n_iter = 20)
# Initialize GloVe
# Here, rank = "dimensionality of embeddings"
# rank = 50 means each word will be represented by a 50-dimensional vector (e.g., 50 numbers)
# x_max = weighting cap; You can ask ChatGPT for more details :)
glove <- GlobalVectors$new(rank = 50, x_max = 10)
# Fit the model to the term co-occurrence matrix
# n_iter controls how many training steps to run (more = better but slower)
word_vectors <- glove$fit_transform(tcm, n_iter = 20)
# The GloVe model gives us two types of embeddings: word vectors and context vectors
# We add them together to get the final representation
context_vectors <- glove$components
word_vectors_final <- word_vectors + t(context_vectors)
# For example: find words similar to "cat"
cat_vec <- word_vectors_final["cat", , drop = FALSE]
cat_vec
# Compute cosine similarity between "cat" and all other words
similarity_scores <- sim2(word_vectors_final, cat_vec, method = "cosine")
# Show top 5 most similar words
head(sort(similarity_scores[,1], decreasing = TRUE), 5)
# Pick a few words from the tiny vocabulary to visualize
words_to_plot <- c("cat", "dog", "dogs", "cats", "mat", "rug", "great")
# Extract their vectors
word_matrix <- word_vectors_final[words_to_plot, ]
# Run t-SNE to reduce 50D vectors into 2D for plotting
tsne <- Rtsne(word_matrix, dims = 2, perplexity = 2)
# Create dataframe for ggplot
tsne_df <- data.frame(x = tsne$Y[,1],
y = tsne$Y[,2],
word = words_to_plot)
# Plot the result
ggplot(tsne_df, aes(x = x, y = y, label = word)) +
geom_text(size = 5) +
ggtitle("t-SNE of Custom Trained Word Embeddings") +
theme_minimal()
