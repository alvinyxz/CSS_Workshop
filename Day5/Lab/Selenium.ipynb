{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium\n",
    "\n",
    "Selenium is a powerful tool for controlling your web browsers and performing automation.\n",
    "\n",
    "You can use it to automate tasks in web browsers, such as filling out forms, clicking buttons, and navigating through pages. Once the content that you want is loaded in a browser, we can invoke other tools such as BeautifulSoup (a Python scraping library) to extract data from web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in /Users/alvinzhou/anaconda3/lib/python3.11/site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in /Users/alvinzhou/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in /Users/alvinzhou/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in /Users/alvinzhou/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alvinzhou/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alvinzhou/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alvinzhou/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alvinzhou/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (2025.4.26)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load necessary libraries\n",
    "! pip install webdriver-manager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to install the Selenium library and use it to control a web browser.\n",
    "\n",
    "# Depend on which browser you want to use, you will run one of the two following trunks (One for Chrome, one for Firefox):\n",
    "\n",
    "**I recommend you to use a browser that you do not do your daily work with, so you can avoid issues with your browser settings and extensions. For example, in my day-to-day work, I use Chrome, so I will install Firefox and just use it for Selenium.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use Firefox, only run this part\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use Chrome, only run this part\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above trunk will open a Firefox browser\n",
    "\n",
    "The next trunk will navigate the browser to the specified URL and do some basic interactions with the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to MyU portal\n",
    "driver.get(\"https://myu.umn.edu\")\n",
    "\n",
    "# Wait for page to load\n",
    "time.sleep(10)\n",
    "\n",
    "# Enter credentials (yes, you will input your username and password here)\n",
    "username = \"\" # Replace with your actual username\n",
    "password = \"\"  # Replace with your actual password\n",
    "# Never do this in public code! You are gonna get hacked!\n",
    "\n",
    "# Fill username and submit\n",
    "driver.find_element(By.ID, \"username\").send_keys(username)\n",
    "driver.find_element(By.ID, \"password\").send_keys(password)\n",
    "driver.find_element(By.NAME, \"_eventId_proceed\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After running the above trunk, you might need to authenticate with Duo or click something on the page, so that it logs you in, which you have to do manually.\n",
    "\n",
    "Now you are going to run the next trunk to navigate to the \"My Info\" tab and extract the data from the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right-click on the My Info tab and inspect it to find the href value\n",
    "# This is usually something like '#tab_UM_SSS_MY_INFORMATION'\n",
    "# The \"click()\" action will click on the My Info tab for you\n",
    "my_info_tab = driver.find_element(By.CSS_SELECTOR, 'a[href=\"#tab_UM_SSS_MY_INFORMATION\"]')\n",
    "driver.execute_script(\"arguments[0].click();\", my_info_tab)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above trunk should have switched your browser to the \"My Info\" tab.\n",
    "\n",
    "Now, we will scrape the data from the My Info tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can right-click on the page and inspect it to find the structure of the HTML\n",
    "\n",
    "# You can see that, your phone number is under id 'UM_SSS_MY_INFO_PHONE_XFORM'\n",
    "phone_number_element = driver.find_element(By.XPATH, '//*[@id=\"UM_SSS_MY_INFO_PHONE_XFORM\"]//td')\n",
    "phone_number = phone_number_element.text\n",
    "print(\"My Phone Number:\", phone_number)\n",
    "\n",
    "# You can see that, your address is under id 'UM_SSS_MY_INFO_ADDRESS_XFORM'\n",
    "address_element = driver.find_element(By.XPATH, '//*[@id=\"UM_SSS_MY_INFO_ADDRESS_XFORM\"]//td')\n",
    "address = address_element.text\n",
    "print(\"My Address:\", address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, you can use `selenium` to do even more crazy things. You can literally control the browser as if you were a human, including clicking buttons, filling out forms, and even taking screenshots.\n",
    "\n",
    "Using `selenium` with other tools such as scraping tools like BeautifulSoup, you can extract data from web pages and automate tasks that would otherwise be tedious and time-consuming.\n",
    "\n",
    "Compared to directly reading in HTML, like what we did in R, this approach \"simulates\" a human user interacting with the browser, which can be useful for websites that require JavaScript to load content or have complex interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Scraping Again\n",
    "\n",
    "Remember the NBA scraping we did in R? We could not do it because the data was loaded dynamically using JavaScript, and we could not get the data directly from the HTML.\n",
    "\n",
    "However, with Selenium, we can automate the browser to load the page and extract the data.\n",
    "\n",
    "Again, between the below two trunks, you will only select one to run, depending on which browser you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use Firefox, only run this part\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you use Chrome, only run this part\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will run the following trunk to get the data from the NBA page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 tables.\n"
     ]
    }
   ],
   "source": [
    "# Go to NBA website\n",
    "driver.get(\"https://www.nba.com/standings\")\n",
    "\n",
    "# Wait for page to load\n",
    "time.sleep(10)\n",
    "\n",
    "# Parse page source with BeautifulSoup (a HTML parser, you can interpret it as a library that helps you read HTML)\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# Get both tables\n",
    "tables = soup.find_all(\"table\", class_=\"Crom_table__p1iZz\")\n",
    "print(f\"Found {len(tables)} tables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the two tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eastern Conference:\n",
      "                     Team   w   L  WIN%  GB   CONF   DIV   HOME   ROAD  \\\n",
      "0  1ClevelandCavaliers- e  64  18  .780  --  41-11  12-4   34-7  30-11   \n",
      "1       2BostonCeltics- a  61  21  .744   3  39-13  14-2  28-13   33-8   \n",
      "2      3New YorkKnicks- x  51  31  .622  13  34-18  12-4  27-14  24-17   \n",
      "3       4IndianaPacers- x  50  32  .610  14  29-22  10-6  29-11  20-20   \n",
      "4      5MilwaukeeBucks- x  48  34  .585  16  31-21   9-7  27-14  20-20   \n",
      "\n",
      "  Neutral   OT LAST10 STREAK  \n",
      "0     0-0  1-1    6-4    L 1  \n",
      "1     0-0  4-2    8-2    W 2  \n",
      "2     0-0  6-2    6-4    W 1  \n",
      "3     1-1  5-1    8-2    W 1  \n",
      "4     1-0  3-0    8-2    W 8  \n",
      "\n",
      "Western Conference:\n",
      "                       Team   w   L  WIN%  GB   CONF   DIV   HOME   ROAD  \\\n",
      "0  1Oklahoma CityThunder- w  68  14  .829  --  39-13  12-4   35-6   32-8   \n",
      "1       2HoustonRockets- sw  52  30  .634  16  31-21  13-3  29-12  23-17   \n",
      "2     3Los AngelesLakers- p  50  32  .610  18  36-16  12-4  31-10  19-22   \n",
      "3         4DenverNuggets- x  50  32  .610  18  32-20   8-8  26-15  24-17   \n",
      "4            5LAClippers- x  50  32  .610  18  29-23   9-7  30-11  20-21   \n",
      "\n",
      "  Neutral   OT LAST10 STREAK  \n",
      "0     1-0  0-1    8-2    W 4  \n",
      "1     0-1  3-1    6-4    L 3  \n",
      "2     0-0  1-1    6-4    L 1  \n",
      "3     0-0  5-1    5-5    W 3  \n",
      "4     0-0  3-2    9-1    W 8  \n"
     ]
    }
   ],
   "source": [
    "# Eastern Conference is first, Western is second\n",
    "eastern_table = tables[0]\n",
    "western_table = tables[1]\n",
    "\n",
    "def parse_table(html_table):\n",
    "    rows = html_table.find_all(\"tr\")\n",
    "    data = []\n",
    "    for row in rows:\n",
    "        cols = [col.get_text(strip=True) for col in row.find_all(['th', 'td'])]\n",
    "        if cols:  # skip empty rows\n",
    "            data.append(cols)\n",
    "    df = pd.DataFrame(data[1:], columns=data[0])  # First row is header\n",
    "    return df\n",
    "\n",
    "# Convert to DataFrames\n",
    "east_df = parse_table(eastern_table)\n",
    "west_df = parse_table(western_table)\n",
    "\n",
    "# Preview results\n",
    "print(\"Eastern Conference:\")\n",
    "print(east_df.head())\n",
    "\n",
    "print(\"\\nWestern Conference:\")\n",
    "print(west_df.head())\n",
    "\n",
    "# Some cleaning is needed, but you see how to get the data now :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: close the driver\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
