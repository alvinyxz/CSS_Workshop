{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc633fc6-13b9-4eba-9859-3e4c1d3a8156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Configuration and Setup\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# Output directory\n",
    "os.makedirs(\"02_Result/images\", exist_ok=True)\n",
    "os.makedirs(\"02_Result\", exist_ok=True)\n",
    "\n",
    "# CSV file path (used both for saving and loading)\n",
    "csv_path = \"02_Result/results.csv\"\n",
    "\n",
    "# Global dictionary to store collected posts.\n",
    "collected_posts = {}\n",
    "\n",
    "# Global list to store a row per image.\n",
    "collected_images = []\n",
    "\n",
    "# If the CSV exists, load its contents into collected_posts so we don't lose prior data.\n",
    "if os.path.exists(csv_path):\n",
    "    try:\n",
    "        df_existing = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
    "        for _, row in df_existing.iterrows():\n",
    "            collected_posts[row['Id']] = row.to_dict()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading existing CSV: {e}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def download_image(image_url: str, post_id: str) -> tuple[str, str]:\n",
    "    try:\n",
    "        image_id = str(uuid.uuid4())\n",
    "        image_extension = image_url.split('.')[-1].split('?')[0]\n",
    "        file_name = f\"02_Result/images/{image_id}.{image_extension}\"\n",
    "        response = requests.get(image_url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            return f\"{image_id}:\", file_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image from {image_url}: {e}\")\n",
    "    return '', ''\n",
    "\n",
    "def process_submission_data(json_data: dict, keyword: str, end_time: int) -> tuple[int, int]:\n",
    "    data = json_data.get('data', [])\n",
    "    if not data:\n",
    "        return 0, None\n",
    "\n",
    "    last_timestamp = None\n",
    "    new_posts_count = 0\n",
    "\n",
    "    for item in data:\n",
    "        created_utc = item.get('created_utc', 0)\n",
    "        if created_utc >= end_time:\n",
    "            continue\n",
    "\n",
    "        post_id = item.get('id', '')\n",
    "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(created_utc))\n",
    "        content = item.get('selftext', '')\n",
    "        if content in ['[removed]', '[deleted]']:\n",
    "            content = ''\n",
    "\n",
    "        image_urls = []\n",
    "        if item.get('gallery_data') and item.get('media_metadata'):\n",
    "            media_metadata = item.get('media_metadata', {})\n",
    "            for media_id, media in media_metadata.items():\n",
    "                if media.get('e') == 'Image':\n",
    "                    mime = media.get('m', '')\n",
    "                    ext = mime.split('/')[-1] if '/' in mime else 'jpg'\n",
    "                    image_urls.append(f'https://i.redd.it/{media_id}.{ext}')\n",
    "        elif item.get('url', '').lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
    "            image_urls.append(item.get('url'))\n",
    "\n",
    "        image_tuples = []\n",
    "        for url in image_urls:\n",
    "            image_id, file_name = download_image(url, post_id)\n",
    "            if image_id:\n",
    "                image_tuples.append((image_id, file_name))\n",
    "\n",
    "        if not image_tuples:\n",
    "            continue\n",
    "\n",
    "        existing_rows = [row for row in collected_images if row['Id'] == post_id]\n",
    "        if existing_rows:\n",
    "            for row in existing_rows:\n",
    "                existing_keywords = set(row['Keyword'].split(','))\n",
    "                if keyword not in existing_keywords:\n",
    "                    existing_keywords.add(keyword)\n",
    "                    row['Keyword'] = ','.join(sorted(existing_keywords))\n",
    "            for image_id, file_name in image_tuples:\n",
    "                if not any(row for row in existing_rows if row.get('Image_ID') == image_id):\n",
    "                    new_row = {\n",
    "                        'Id': post_id,\n",
    "                        'Keyword': existing_rows[0]['Keyword'],\n",
    "                        'Timestamp': timestamp,\n",
    "                        'Content': content,\n",
    "                        'Author': item.get('author', ''),\n",
    "                        'Post_url': 'https://www.reddit.com' + item.get('permalink', ''),\n",
    "                        'Image_ID': image_id,\n",
    "                        'File': file_name,\n",
    "                        'Upvotes': item.get('ups', 0),\n",
    "                        'Downvotes': item.get('downs', 0),\n",
    "                        'Comments': item.get('num_comments', 0)\n",
    "                    }\n",
    "                    collected_images.append(new_row)\n",
    "        else:\n",
    "            for image_id, file_name in image_tuples:\n",
    "                new_row = {\n",
    "                    'Id': post_id,\n",
    "                    'Keyword': keyword,\n",
    "                    'Timestamp': timestamp,\n",
    "                    'Content': content,\n",
    "                    'Author': item.get('author', ''),\n",
    "                    'Post_url': 'https://www.reddit.com' + item.get('permalink', ''),\n",
    "                    'Image_ID': image_id,\n",
    "                    'File': file_name,\n",
    "                    'Upvotes': item.get('ups', 0),\n",
    "                    'Downvotes': item.get('downs', 0),\n",
    "                    'Comments': item.get('num_comments', 0)\n",
    "                }\n",
    "                collected_images.append(new_row)\n",
    "            new_posts_count += 1\n",
    "\n",
    "        post_rows = [row for row in collected_images if row['Id'] == post_id]\n",
    "        total_images = len(post_rows)\n",
    "        for row in post_rows:\n",
    "            row['Num_Images'] = total_images\n",
    "\n",
    "        last_timestamp = created_utc\n",
    "\n",
    "    return new_posts_count, last_timestamp\n",
    "\n",
    "def save_to_csv(csv_path: str) -> None:\n",
    "    df = pd.DataFrame(collected_images)\n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Keyword Processing Function\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def process_keyword(keyword: str, start_time: int, end_time: int, size: int = 100) -> None:\n",
    "    base_url = \"https://api.pullpush.io/reddit/search/submission/\"\n",
    "    session = requests.Session()\n",
    "\n",
    "    current_day = start_time\n",
    "    while current_day < end_time:\n",
    "        day_start = current_day\n",
    "        day_end = current_day + 86400\n",
    "        day_str = time.strftime(\"%Y-%m-%d\", time.localtime(day_start))\n",
    "        print(f\"{keyword} - {day_str}\")\n",
    "\n",
    "        day_current_after = day_start\n",
    "        while True:\n",
    "            params = {\n",
    "                'q': keyword,\n",
    "                'after': day_current_after,\n",
    "                'before': day_end,\n",
    "                'sort': 'asc',\n",
    "                'sort_type': 'created_utc',\n",
    "                'size': size,\n",
    "            }\n",
    "            try:\n",
    "                response = session.get(base_url, params=params, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    time.sleep(2)\n",
    "                    continue\n",
    "\n",
    "                json_data = response.json()\n",
    "                num_results = len(json_data.get('data', []))\n",
    "\n",
    "                new_count, last_timestamp = process_submission_data(json_data, keyword, day_end)\n",
    "\n",
    "                if num_results < size or last_timestamp is None:\n",
    "                    break\n",
    "\n",
    "                day_current_after = int(last_timestamp) + 1\n",
    "                time.sleep(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing keyword {keyword} on {day_str}: {e}\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "\n",
    "        save_to_csv(csv_path)\n",
    "        current_day = day_end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6567b53-0ee3-49a0-b70e-6fadb149f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Main Execution Block\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = \"2024-07-14 00:00:00\"\n",
    "    end = \"2025-05-14 00:00:00\"\n",
    "    start_time_epoch = int(time.mktime(time.strptime(start, \"%Y-%m-%d %H:%M:%S\")))\n",
    "    end_time_epoch = int(time.mktime(time.strptime(end, \"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "    keywords = [\n",
    "        # Democrats\n",
    "        'Bernie', 'Sanders',\n",
    "        'Alexandria', 'Ocasio-Cortez', 'Ocasio', 'Cortez', 'AOC',\n",
    "        'Kamala', 'Harris',\n",
    "        'Gavin', 'Newsom',\n",
    "        'Elizabeth', 'Warren',\n",
    "        'Cory', 'Booker',\n",
    "        'Nancy', 'Pelosi',\n",
    "        'Charles', 'Schumer',\n",
    "        'Kamala', 'Harris',\n",
    "        # Republicans\n",
    "        'Donald', 'Trump',\n",
    "        'Elon', 'Musk',\n",
    "        'JD', 'Vance',\n",
    "        'Ron', 'DeSantis',\n",
    "        'Marco', 'Rubio',\n",
    "        'Ted', 'Cruz',\n",
    "        'Marjorie', 'Greene'\n",
    "    ]\n",
    "\n",
    "    for keyword in tqdm(keywords, desc=\"Processing Keywords\"):\n",
    "        print(f\"Processing keyword: {keyword}\")\n",
    "        process_keyword(keyword, start_time_epoch, end_time_epoch)\n",
    "\n",
    "    save_to_csv(csv_path)\n",
    "    print(f\"Final data saved to CSV with {len(collected_images)} image rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b65c5c-01bc-4ed2-b372-9775498fcbf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
