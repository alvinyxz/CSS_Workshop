{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c883c92f-fcb1-4e75-b22f-7ff9f5fa2128",
   "metadata": {
    "id": "16de8cc8-fe8c-4dec-a58a-68fa050b7d3a",
    "outputId": "ac71a964-ee35-4310-e960-fac87d81d30c"
   },
   "source": [
    "# DEMO CLIP\n",
    "\n",
    "#### Author Jiacheng Huang\n",
    "\n",
    "In this tutorial, we will cover the following basics for labeling and fine-tuning multimodal data using the CLIP module from LabelGenius (https://github.com/mediaccs/LabelGenius):\n",
    "\n",
    "1. Labeling image data with CLIP\n",
    "2. Labeling text data with CLIP\n",
    "3. Combining text and image inputs for multimodal classification\n",
    "4. Fine-tuning CLIP for improved task-specific performance\n",
    "\n",
    "\n",
    "#### Cite LabelGenius\n",
    "Huang, J., Zhang, Z., & Su, C. C. (2025). LabelGenius: A Tool for Dynamic and Flexible Labeling for LLM-based Multimodal Content Labeling [Computer software]. GitHub. https://github.com/mediaccs/LabelGenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe1770b-4fdb-4577-9a83-ee71fc0f2d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install labelgenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2200e6-2a4a-4eed-b0d8-32b7377f8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the functions\n",
    "import pandas as pd\n",
    "from labelgenius import classification_CLIP_0_shot, finetune_CLIP, classification_CLIP_finetuned, auto_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406856d6-e095-4b66-813e-ec0f579e7043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or import from the direct folder\n",
    "import os\n",
    "LabelGenius_path = os.path.expanduser(\"LabelGenius-main/\")\n",
    "sys.path.append(LabelGenius_path)\n",
    "from labelgenius import classification_CLIP_0_shot, finetune_CLIP, classification_CLIP_finetuned, auto_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e482d-b8af-47f1-94ce-5d3820dbdf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to the folder contain the data\n",
    "# import os\n",
    "# os.chdir('/Users/jiacheng/Desktop/LLM/DEMO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-apache",
   "metadata": {
    "id": "ecdea590-13db-4d21-b105-9948c1ff3b95"
   },
   "source": [
    "Demo 1: Single-Category Classification using N24News Dataset\n",
    "-------------------------------------------------------------\n",
    "\n",
    "This demo shows how to classify a single news article into one of 24 category\n",
    "using the N24News dataset. Each article in the dataset includes both textual\n",
    "and visual information.\n",
    "\n",
    "Source: https://aclanthology.org/2022.lrec-1.729/\n",
    "\n",
    "\n",
    "Each article contains the following fields:\n",
    "- 'section': Ground truth label (one of 24 category)\n",
    "- 'headline': Title of the article\n",
    "- 'abstract': Short summary of the article\n",
    "- 'article': Full text content\n",
    "- 'article_url': Link to the original article\n",
    "- 'image': Encoded image or metadata (optional)\n",
    "- 'caption': Image caption\n",
    "- 'image_id': Unique image identifier\n",
    "- 'image_path': Path to the associated image (e.g., 'N24News/imgs_200_sample1/12345.jpg')\n",
    "- 'article_id': Unique article identifier\n",
    "\n",
    "Image file: Multimodal_image\n",
    "\n",
    "Example category (See prompt_D1 for the complete category):\n",
    "------------------------\n",
    "1. Health\n",
    "2. Science\n",
    "3. Television\n",
    "...\n",
    "24. Global Business\n",
    "\n",
    "Reference:\n",
    "----------\n",
    "Wang, Z., Shan, X., Zhang, X., & Yang, J. (2022).\n",
    "N24News: A New Dataset for Multimodal News Classification.\n",
    "In *Proceedings of the Thirteenth Language Resources and Evaluation Conference* (pp. 6768â€“6775). LREC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-commander",
   "metadata": {
    "id": "708f68b3-8d41-4647-8d86-eb282e65a53f"
   },
   "source": [
    "### Demo 1a: Single-Category Text Classification\n",
    "\n",
    "**Datasets:**\n",
    "- `D1_1.csv`: Used for initial labeling and fine-tuning.\n",
    "- `D1_2.csv`: Used for testing the fine-tuned model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ffb6c5-73dc-45ee-bb67-e620f23daa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the descriptive prompt for each category\n",
    "prompt_D1_CLIP = [\n",
    "    \"a news article about health, including medical news, public health issues, fitness, mental health, and wellness.\",\n",
    "    \"a news article about science, covering scientific discoveries, research studies, space exploration, and innovations.\",\n",
    "    \"a news article about television, featuring TV shows, series reviews, industry news, and streaming platforms.\",\n",
    "    \"a news article about travel, focusing on tourism, destinations, travel guides, airlines, and vacation trends.\",\n",
    "    \"a news article about movies, including film industry news, reviews, box office reports, and upcoming releases.\",\n",
    "    \"a news article about dance, covering ballet, contemporary styles, street dance, performances, and dance competitions.\",\n",
    "    \"a news article about real estate, highlighting housing market trends, property sales, architecture, and urban planning.\",\n",
    "    \"a news article about the economy, featuring macroeconomics, inflation, stock markets, GDP growth, and financial policies.\",\n",
    "    \"a news article about sports, covering professional sports, competitions, athlete news, and game Demo_results.\",\n",
    "    \"a news article about theater, featuring plays, Broadway shows, live performances, and stage production reviews.\",\n",
    "    \"a news article about opinion pieces, including editorials, analysis, and expert commentaries.\",\n",
    "    \"a news article about music, covering albums, artists, concerts, festivals, and industry trends.\",\n",
    "    \"a news article about books, featuring literature, bestsellers, author interviews, and book reviews.\",\n",
    "    \"a news article about art and design, showcasing fine arts, visual arts, museums, exhibitions, and design trends.\",\n",
    "    \"a news article about style, including fashion trends, beauty, personal style, and cultural aesthetics.\",\n",
    "    \"a news article about media, covering journalism, publishing, digital media, and mass communication.\",\n",
    "    \"a news article about food, featuring restaurants, cooking, recipes, culinary trends, and food culture.\",\n",
    "    \"a news article about well-being, focusing on lifestyle, personal development, mental well-being, and self-care.\",\n",
    "    \"a news article about fashion, covering clothing, designers, fashion weeks, and industry insights.\",\n",
    "    \"a news article about technology, featuring AI, gadgets, software, cybersecurity, and tech innovations.\",\n",
    "    \"a news article about personal finance, including investing, budgeting, and financial planning.\",\n",
    "    \"a news article about education, featuring schools, universities, learning methods, and education policies.\",\n",
    "    \"a news article about automobiles, covering car industry news, electric vehicles, reviews, and trends.\",\n",
    "    \"a news article about global business, featuring international trade, corporations, mergers, and global markets.\"\n",
    "]\n",
    "\n",
    "# Define the list of 24 category labels\n",
    "category_D1_CLIP = [\n",
    "    \"1\", \"2\", \"3\", \"4\", \"5\", \"6\",\n",
    "    \"7\", \"8\", \"9\", \"10\", \"11\", \"12\",\n",
    "    \"13\", \"14\", \"15\", \"16\", \"17\", \"18\",\n",
    "    \"19\", \"20\", \"21\", \"22\", \"23\", \"24\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-voltage",
   "metadata": {},
   "source": [
    "## CLIP: local labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-corruption",
   "metadata": {
    "id": "fc615e82-c25c-43d4-893e-e246d254ab21",
    "outputId": "b7f49318-538a-4f27-88f2-efe8d1a67d78"
   },
   "outputs": [],
   "source": [
    "D1a_CLIP_inital_labeling = classification_CLIP_0_shot(\n",
    "    text_path=\"Demo_data/D1_1.csv\",\n",
    "    mode=\"text\",\n",
    "    prompt=prompt_D1_CLIP,\n",
    "    text_column=[\"headline\", \"abstract\"],\n",
    "    predict_column=\"D1a_CLIP_inital_labeling\",\n",
    ")\n",
    "\n",
    "\n",
    "D1a_CLIP_inital_labeling.to_csv(\"Demo_result/D1a_CLIP_inital_labeling.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6445d79-27d1-4327-b7ce-6e0e0e98bb55",
   "metadata": {},
   "source": [
    "### Let's compare the CLIP label with the ground-truth human label (\"section_numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-control",
   "metadata": {
    "id": "a37c5284-3059-4aeb-80df-435bda86b510",
    "outputId": "6645fe3c-dcfd-40fc-cda9-3db9337a358e"
   },
   "outputs": [],
   "source": [
    "## Check the accuracy: CLIP initial labeling\n",
    "auto_verification(\n",
    "    D1a_CLIP_inital_labeling,\n",
    "    predicted_cols=\"D1a_CLIP_inital_labeling\",\n",
    "    true_cols=\"section_numeric\",\n",
    "    category=category_D1_CLIP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2633a28f-2b83-4cde-8775-041d9f380a20",
   "metadata": {},
   "source": [
    "### Okay, the accuracy of the text-only data is 55%. Let's try to fine-tune it based on the true column \"section_numeric\".\n",
    "\n",
    "#### Note. This demo only fine-tunes the first 20 records, so the result after fine-tuning could be worse. Usually, finetuning for CLIP works for 1000+ records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-brighton",
   "metadata": {},
   "source": [
    "### finetune: CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-adelaide",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finetune CLIP\n",
    "finetune_CLIP(\n",
    "    mode=\"text\",\n",
    "    text_path=\"Demo_data/D1_1.csv\",\n",
    "    true_label=\"section_numeric\",\n",
    "    model_name=\"Demo_finetuned_CLIP/D1a_CLIP_model_finetuned.pth\",\n",
    "    num_epochs=20,\n",
    "    batch_size=8,\n",
    "    learning_rate=1e-5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify with a fineâ€‘tuned model\n",
    "D1a_CLIP_finetuned = classification_CLIP_finetuned(\n",
    "    mode=\"text\",\n",
    "    text_path=\"Demo_data/D1_2.csv\",\n",
    "    text_column=[\"headline\", \"abstract\"],\n",
    "    model_name=\"Demo_finetuned_CLIP/D1a_CLIP_model_finetuned.pth\",\n",
    "    predict_column=\"D1a_CLIP_finetuned\",\n",
    ")\n",
    "\n",
    "\n",
    "D1a_CLIP_finetuned.to_csv(\"Demo_result/D1a_CLIP_finetuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the accuracy: CLIP after finetune\n",
    "auto_verification(\n",
    "    D1a_CLIP_finetuned,\n",
    "    predicted_cols=\"D1a_CLIP_finetuned\",\n",
    "    true_cols=\"section_numeric\",\n",
    "    category=category_D1_CLIP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b8ef5b-6cd5-44b5-8057-67bcbf915214",
   "metadata": {},
   "source": [
    "### Not surprisingly, the accuracy after fine-tuning is worse. \n",
    "### If you want to see how finetune improve performance with larger dataset, please look at the demo provided on our GitHub, which was fine-tuned on a large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-motel",
   "metadata": {
    "id": "d6e7cf88-f625-4bdf-adb3-9c2a0d3d0532"
   },
   "source": [
    "# Demo 1b: Classify single-category image data\n",
    "\n",
    "**Datasets:**\n",
    "\n",
    "- `D1_1.csv`: Used for initial labeling and fine-tuning.\n",
    "- `imgs_40_1`: Used for initial labeling and fine-tuning.\n",
    "\n",
    "- `D1_2.csv`: Used for testing the fine-tuned model's performance.\n",
    "- `imgs_40_2`: Used for testing the fine-tuned model's performance.\n",
    "\n",
    "\n",
    "### CSV File Requirements\n",
    "\n",
    "Each CSV file must include at minimum:\n",
    "\n",
    "- `image_id`  \n",
    "  â€“ The base filename (without extension) of each image in the corresponding folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-policy",
   "metadata": {},
   "source": [
    "## CLIP: local labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1b_CLIP_inital_labeling = classification_CLIP_0_shot(\n",
    "    text_path=\"Demo_data/D1_1.csv\",\n",
    "    img_dir=\"Demo_data/D1_imgs_1\",\n",
    "    mode=\"image\",\n",
    "    prompt=prompt_D1_CLIP,\n",
    "    predict_column=\"D1b_CLIP_inital_labeling\",\n",
    ")\n",
    "\n",
    "\n",
    "D1b_CLIP_inital_labeling.to_csv(\"Demo_result/D1b_CLIP_inital_labeling.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc49f70-5798-4c9f-9dbe-a9ea6191939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1b_CLIP_inital_labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the accuracy: CLIP initial labeling\n",
    "auto_verification(\n",
    "    D1b_CLIP_inital_labeling,\n",
    "    predicted_cols=\"D1b_CLIP_inital_labeling\",\n",
    "    true_cols=\"section_numeric\",\n",
    "    category=category_D1_CLIP\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e24ec41-2205-4dcc-9e50-d29c5aa5b08b",
   "metadata": {},
   "source": [
    "### finetune: CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-indian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune CLIP\n",
    "finetune_CLIP(\n",
    "    mode=\"text\",\n",
    "    text_path=\"Demo_data/D1_1.csv\",\n",
    "    true_label=\"section_numeric\",\n",
    "    model_name=\"Demo_finetuned_CLIP/D1b_CLIP_model_finetuned.pth\",\n",
    "    num_epochs=20,\n",
    "    batch_size=8,\n",
    "    learning_rate=1e-5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify with a fineâ€‘tuned model\n",
    "D1b_CLIP_finetuned = classification_CLIP_finetuned(\n",
    "    mode=\"image\",\n",
    "    text_path=\"Demo_data/D1_2.csv\",\n",
    "    img_dir=\"Demo_data/D1_imgs_2\",\n",
    "    model_name=\"Demo_finetuned_CLIP/D1b_CLIP_model_finetuned.pth\",\n",
    "    predict_column=\"D1b_CLIP_finetuned\",\n",
    ")\n",
    "\n",
    "D1b_CLIP_finetuned.to_csv(\"Demo_result/D1b_CLIP_finetuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the accuracy: CLIP after finetune\n",
    "# Merge the Demo_result of the image with the human label\n",
    "auto_verification(\n",
    "    D1b_CLIP_finetuned,\n",
    "    predicted_cols=\"D1b_CLIP_finetuned\",\n",
    "    true_cols=\"section_numeric\",\n",
    "    category=category_D1_CLIP\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-fisher",
   "metadata": {},
   "source": [
    "# Demo 1c: Classify single-category text + image data\n",
    "\n",
    "**Datasets:**\n",
    "- `D1_1.csv`: Used for initial labeling and fine-tuning.\n",
    "- `D1_imgs_1`: Used for initial labeling and fine-tuning.\n",
    "\n",
    "- `D1_2.csv`: Used for testing the fine-tuned model's performance.\n",
    "- `D1_imgs_2`: Used for testing the fine-tuned model's performance.\n",
    "\n",
    "\n",
    "The text dataset should contain a column `image_path` to map the images for each row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-sender",
   "metadata": {},
   "source": [
    "## CLIP: local labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1c_CLIP_inital_labeling = classification_CLIP_0_shot(\n",
    "    text_path=\"Demo_data/D1_1.csv\",\n",
    "    img_dir=\"Demo_data/D1_imgs_1\",\n",
    "    mode=\"both\",\n",
    "    prompt=prompt_D1_CLIP,\n",
    "    text_column=[\"headline\", \"abstract\"],\n",
    "    predict_column=\"D1c_CLIP_inital_labeling\",\n",
    ")\n",
    "\n",
    "D1c_CLIP_inital_labeling.to_csv(\"Demo_result/D1c_CLIP_inital_labeling.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the accuracy: CLIP initial labeling\n",
    "auto_verification(\n",
    "    D1c_CLIP_inital_labeling,\n",
    "    predicted_cols=\"D1c_CLIP_inital_labeling\",\n",
    "    true_cols=\"section_numeric\",\n",
    "    category=category_D1_CLIP\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c8f93a-a358-4abd-ba60-68739d6048a7",
   "metadata": {},
   "source": [
    "### finetune: CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune CLIP\n",
    "finetune_CLIP(\n",
    "    mode=\"both\",\n",
    "    text_path=\"Demo_data/D1_1.csv\",\n",
    "    text_column=[\"headline\", \"abstract\"],\n",
    "    img_dir=\"Demo_data/D1_imgs_1\",\n",
    "    true_label=\"section_numeric\",\n",
    "    model_name=\"Demo_finetuned_CLIP/D1c_CLIP_model_finetuned.pth\",\n",
    "    num_epochs=20,\n",
    "    batch_size=8,\n",
    "    learning_rate=1e-5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify with a fineâ€‘tuned model\n",
    "D1c_CLIP_finetuned = classification_CLIP_finetuned(\n",
    "    mode=\"both\",\n",
    "    text_path=\"Demo_data/D1_2.csv\",\n",
    "    text_column=[\"headline\", \"abstract\"],\n",
    "    img_dir=\"Demo_data/D1_imgs_2\",\n",
    "    model_name=\"Demo_finetuned_CLIP/D1c_CLIP_model_finetuned.pth\",\n",
    "    predict_column=\"D1c_CLIP_finetuned\",\n",
    "    \n",
    ")\n",
    "D1c_CLIP_finetuned.to_csv(\"Demo_result/D1c_CLIP_finetuned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f0583-919d-4064-91a0-bdaa08109746",
   "metadata": {},
   "outputs": [],
   "source": [
    "D1c_CLIP_finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the accuracy: CLIP after finetune\n",
    "auto_verification(\n",
    "    D1c_CLIP_finetuned,\n",
    "    predicted_cols=\"D1c_CLIP_finetuned\",\n",
    "    true_cols=\"section_numeric\",\n",
    "    category=category_D1_CLIP\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
